{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Import custom modules\n",
    "from src.models.bert_model import TrollDetector\n",
    "from src.models.trainer import TrollDetectorTrainer\n",
    "from src.data_tools.dataset import TrollDataset, collate_batch\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "model_name: distilbert-base-multilingual-cased\n",
      "adapter_path: None\n",
      "max_length: 96\n",
      "batch_size: 8\n",
      "learning_rate: 2e-05\n",
      "weight_decay: 0.03\n",
      "num_epochs: 3\n",
      "dropout_rate: 0.1\n",
      "warmup_steps: 50\n",
      "max_grad_norm: 1.0\n",
      "comments_per_user: 10\n",
      "early_stopping_patience: 3\n",
      "random_state: 42\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "DATA_DIR = Path('data')\n",
    "PROCESSED_DATA_DIR = DATA_DIR / 'processed'\n",
    "CHECKPOINT_DIR = Path('./checkpoints')\n",
    "\n",
    "# Create checkpoint directory\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Updated training configuration\n",
    "config = {\n",
    "    'model_name': 'distilbert-base-multilingual-cased',\n",
    "    'adapter_path': None, #Dont use adapter for first training\n",
    "    # 'model_name': 'ufal/robeczech-base',\n",
    "    'max_length': 96,\n",
    "    'batch_size': 8,\n",
    "    'learning_rate': 2e-5,\n",
    "    'weight_decay': 0.03,\n",
    "    'num_epochs': 3,\n",
    "    'dropout_rate': 0.1,\n",
    "    'warmup_steps': 50,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'comments_per_user': 10,\n",
    "    'early_stopping_patience': 3,\n",
    "    'random_state': 17,\n",
    "}\n",
    "\n",
    "# Try to load preprocessing config\n",
    "try:\n",
    "    with open(PROCESSED_DATA_DIR / 'preprocessing_config.json', 'r') as f:\n",
    "        preproc_config = json.load(f)\n",
    "        config['random_state'] = preproc_config.get('random_state', 42)\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: preprocessing_config.json not found, using default random_state\")\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 227740 samples, 8953 authors\n",
      "Val:   57083 samples, 1919 authors\n",
      "Test:  62796 samples, 1919 authors\n"
     ]
    }
   ],
   "source": [
    "# # Load preprocessed data splits\n",
    "# train_df = pd.read_parquet(PROCESSED_DATA_DIR / 'train.parquet')\n",
    "# val_df = pd.read_parquet(PROCESSED_DATA_DIR / 'val.parquet')\n",
    "# test_df = pd.read_parquet(PROCESSED_DATA_DIR / 'test.parquet')\n",
    "\n",
    "# # Load preprocessed small data splits\n",
    "train_df = pd.read_parquet(PROCESSED_DATA_DIR / 'train.parquet')\n",
    "val_df = pd.read_parquet(PROCESSED_DATA_DIR / 'val.parquet')\n",
    "test_df = pd.read_parquet(PROCESSED_DATA_DIR / 'test.parquet')\n",
    "\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"Train: {len(train_df)} samples, {train_df['author'].nunique()} authors\")\n",
    "print(f\"Val:   {len(val_df)} samples, {val_df['author'].nunique()} authors\")\n",
    "print(f\"Test:  {len(test_df)} samples, {test_df['author'].nunique()} authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_tools.dataset:Using 'text' as text column\n",
      "INFO:src.data_tools.dataset:Labels are already normalized between 0 and 1\n",
      "INFO:src.data_tools.dataset:Created 20362 samples from 8953 authors\n",
      "INFO:src.data_tools.dataset:Using 'text' as text column\n",
      "INFO:src.data_tools.dataset:Labels are already normalized between 0 and 1\n",
      "INFO:src.data_tools.dataset:Created 4274 samples from 1919 authors\n",
      "INFO:src.data_tools.dataset:Using 'text' as text column\n",
      "INFO:src.data_tools.dataset:Labels are already normalized between 0 and 1\n",
      "INFO:src.data_tools.dataset:Created 4468 samples from 1919 authors\n"
     ]
    }
   ],
   "source": [
    "# Create Datasets with regression settings\n",
    "train_dataset = TrollDataset(\n",
    "    train_df,\n",
    "    tokenizer_name=config['model_name'],\n",
    "    max_length=config['max_length'],\n",
    "    comments_per_user=config['comments_per_user'],\n",
    "    label_column='troll',  # or your trolliness score column\n",
    "    normalize_labels=True  # This will automatically normalize scores to [0,1]\n",
    ")\n",
    "\n",
    "val_dataset = TrollDataset(\n",
    "    val_df,\n",
    "    tokenizer_name=config['model_name'],\n",
    "    max_length=config['max_length'],\n",
    "    comments_per_user=config['comments_per_user'],\n",
    "    label_column='troll',\n",
    "    normalize_labels=True\n",
    ")\n",
    "\n",
    "test_dataset = TrollDataset(\n",
    "    test_df,\n",
    "    tokenizer_name=config['model_name'],\n",
    "    max_length=config['max_length'],\n",
    "    comments_per_user=config['comments_per_user'],\n",
    "    label_column='troll',\n",
    "    normalize_labels=True\n",
    ")\n",
    "\n",
    "# Create dataloaders (unchanged)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
