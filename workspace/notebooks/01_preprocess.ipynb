{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "780d88a7",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# declare a list tasks whose products you want to use as inputs\n",
    "upstream = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5e452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from src.data_tools.preprocessor import load_and_clean_data\n",
    "from src.data_tools.dataset import create_data_splits\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08b89778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# Define paths\n",
    "DATA_DIR = Path('data')\n",
    "PROCESSED_DATA_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model configuration\n",
    "config = {\n",
    "    'train_size': 0.7,\n",
    "    'val_size': 0.15,\n",
    "    'test_size': 0.15,\n",
    "    'random_state': 42\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d38b39b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_tools.preprocessor:Loading Russian troll tweets...\n",
      "INFO:src.data_tools.preprocessor:Loading Sentiment140 tweets...\n",
      "INFO:src.data_tools.preprocessor:Loading celebrity tweets...\n",
      "INFO:src.data_tools.preprocessor:Loading manualy scraped tweets...\n",
      "INFO:src.data_tools.preprocessor:Loading Twitter JSON data from non_troll_politics folder...\n",
      "INFO:src.data_tools.preprocessor:Loading information operations tweets...\n",
      "INFO:src.data_tools.preprocessor:Found 151 parquet files in information_operations folder and its subdirectories\n",
      "INFO:src.data_tools.preprocessor:Information operations data distribution - Trolls: 585379, Non-trolls: 914607\n",
      "INFO:src.data_tools.preprocessor:Loading data collected by Machova...\n",
      "INFO:src.data_tools.preprocessor:Combining datasets...\n",
      "INFO:src.data_tools.preprocessor:Filtering accounts with few tweets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset info:\n",
      "Total samples: 831959\n",
      "Columns: ['author', 'text', 'troll', 'language']\n",
      "\n",
      "Missing values:\n",
      "author         0\n",
      "text           0\n",
      "troll          0\n",
      "language    1748\n",
      "dtype: int64\n",
      "\n",
      "Class distribution:\n",
      "troll\n",
      "0    0.826238\n",
      "1    0.173762\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load Czech comment data\n",
    "df_raw = load_and_clean_data(DATA_DIR, max_tweets_per_source=10000, max_tweets_per_author=50)\n",
    "\n",
    "df_raw = df_raw.rename(columns={'account': 'author'})\n",
    "df_raw = df_raw.rename(columns={'tweet': 'text'})\n",
    "\n",
    "print(\"Raw dataset info:\")\n",
    "print(f\"Total samples: {len(df_raw)}\")\n",
    "print(f\"Columns: {df_raw.columns.tolist()}\")\n",
    "print(\"\\nMissing values:\")\n",
    "print(df_raw.isnull().sum())\n",
    "\n",
    "# Show class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df_raw['troll'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8b8142c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet distribution:\n",
      "Troll tweets: 144,563 (17.4%)\n",
      "Non-troll tweets: 687,396 (82.6%)\n",
      "Total tweets: 831,959\n",
      "\n",
      "Author statistics:\n",
      "Troll authors: 4,555\n",
      "Non-troll authors: 41,182\n",
      "Average tweets per troll author: 31.7\n",
      "Average tweets per non-troll author: 16.7\n"
     ]
    }
   ],
   "source": [
    "# Count tweets by troll label and author stats\n",
    "tweet_counts = df_raw['troll'].value_counts()\n",
    "total_tweets = len(df_raw)\n",
    "\n",
    "# Get author stats split by troll/non-troll\n",
    "author_stats = df_raw.groupby(['author', 'troll']).size().reset_index()\n",
    "troll_authors = author_stats[author_stats['troll'] == 1]['author'].nunique()\n",
    "nontroll_authors = author_stats[author_stats['troll'] == 0]['author'].nunique()\n",
    "\n",
    "# Calculate average tweets per author type\n",
    "troll_tweets_per_author = df_raw[df_raw['troll'] == 1].groupby('author').size()\n",
    "nontroll_tweets_per_author = df_raw[df_raw['troll'] == 0].groupby('author').size()\n",
    "avg_troll_tweets = troll_tweets_per_author.mean()\n",
    "avg_nontroll_tweets = nontroll_tweets_per_author.mean()\n",
    "\n",
    "print(\"Tweet distribution:\")\n",
    "print(f\"Troll tweets: {tweet_counts[1]:,} ({tweet_counts[1]/total_tweets:.1%})\")\n",
    "print(f\"Non-troll tweets: {tweet_counts[0]:,} ({tweet_counts[0]/total_tweets:.1%})\")\n",
    "print(f\"Total tweets: {total_tweets:,}\")\n",
    "print(f\"\\nAuthor statistics:\")\n",
    "print(f\"Troll authors: {troll_authors:,}\")\n",
    "print(f\"Non-troll authors: {nontroll_authors:,}\")\n",
    "print(f\"Average tweets per troll author: {avg_troll_tweets:.1f}\")\n",
    "print(f\"Average tweets per non-troll author: {avg_nontroll_tweets:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "126d4ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After resampling non-troll authors to 20%:\n",
      "Total samples: 282903\n",
      "Total authors: 12791\n",
      "\n",
      "Class distribution:\n",
      "troll\n",
      "1    0.510998\n",
      "0    0.489002\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Resample non-troll authors to only keep 20%\n",
    "non_troll_authors = df_raw[df_raw['troll'] == 0]['author'].unique()\n",
    "sampled_non_troll_authors = np.random.choice(\n",
    "    non_troll_authors,\n",
    "    size=int(len(non_troll_authors) * 0.2),\n",
    "    replace=False\n",
    ")\n",
    "\n",
    "# Keep all troll authors and only sampled non-troll authors\n",
    "troll_tweets = df_raw[df_raw['troll'] == 1]\n",
    "sampled_non_troll_tweets = df_raw[\n",
    "    (df_raw['troll'] == 0) & \n",
    "    (df_raw['author'].isin(sampled_non_troll_authors))\n",
    "]\n",
    "df_raw = pd.concat([troll_tweets, sampled_non_troll_tweets])\n",
    "\n",
    "print(\"\\nAfter resampling non-troll authors to 20%:\")\n",
    "print(f\"Total samples: {len(df_raw)}\")\n",
    "print(f\"Total authors: {df_raw['author'].nunique()}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df_raw['troll'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63e94d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet distribution:\n",
      "Troll tweets: 144,563 (51.1%)\n",
      "Non-troll tweets: 138,340 (48.9%)\n",
      "Total tweets: 282,903\n",
      "\n",
      "Author statistics:\n",
      "Troll authors: 4,555 (35.6%)\n",
      "Non-troll authors: 8,236 (64.4%)\n",
      "Average tweets per troll author: 31.7\n",
      "Average tweets per non-troll author: 16.8\n"
     ]
    }
   ],
   "source": [
    "# Count tweets by troll label and author stats\n",
    "tweet_counts = df_raw['troll'].value_counts()\n",
    "total_tweets = len(df_raw)\n",
    "\n",
    "# Get author stats split by troll/non-troll\n",
    "author_stats = df_raw.groupby(['author', 'troll']).size().reset_index()\n",
    "troll_authors = author_stats[author_stats['troll'] == 1]['author'].nunique()\n",
    "nontroll_authors = author_stats[author_stats['troll'] == 0]['author'].nunique()\n",
    "\n",
    "# Calculate average tweets per author type\n",
    "troll_tweets_per_author = df_raw[df_raw['troll'] == 1].groupby('author').size()\n",
    "nontroll_tweets_per_author = df_raw[df_raw['troll'] == 0].groupby('author').size()\n",
    "avg_troll_tweets = troll_tweets_per_author.mean()\n",
    "avg_nontroll_tweets = nontroll_tweets_per_author.mean()\n",
    "\n",
    "print(\"Tweet distribution:\")\n",
    "print(f\"Troll tweets: {tweet_counts[1]:,} ({tweet_counts[1]/total_tweets:.1%})\")\n",
    "print(f\"Non-troll tweets: {tweet_counts[0]:,} ({tweet_counts[0]/total_tweets:.1%})\")\n",
    "print(f\"Total tweets: {total_tweets:,}\")\n",
    "print(f\"\\nAuthor statistics:\")\n",
    "print(f\"Troll authors: {troll_authors:,} ({troll_authors/(troll_authors+nontroll_authors):.1%})\")\n",
    "print(f\"Non-troll authors: {nontroll_authors:,} ({nontroll_authors/(troll_authors+nontroll_authors):.1%})\")\n",
    "print(f\"Average tweets per troll author: {avg_troll_tweets:.1f}\")\n",
    "print(f\"Average tweets per non-troll author: {avg_nontroll_tweets:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "928b8683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits:\n",
      "Train: 198385 samples, 8953 authors\n",
      "Val:   42099 samples, 1919 authors\n",
      "Test:  42419 samples, 1919 authors\n"
     ]
    }
   ],
   "source": [
    "# Create Train/Val/Test Splits\n",
    "# Create splits ensuring no author overlap\n",
    "train_df, val_df, test_df = create_data_splits(\n",
    "    df_raw,\n",
    "    train_size=config['train_size'],\n",
    "    val_size=config['val_size'],\n",
    "    test_size=config['test_size'],\n",
    "    random_state=config['random_state']\n",
    ")\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "print(f\"Train: {len(train_df)} samples, {train_df['author'].nunique()} authors\")\n",
    "print(f\"Val:   {len(val_df)} samples, {val_df['author'].nunique()} authors\")\n",
    "print(f\"Test:  {len(test_df)} samples, {test_df['author'].nunique()} authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f05ff77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train_max_50 split to data/processed/train_max_50.parquet\n",
      "Saved val_max_50 split to data/processed/val_max_50.parquet\n",
      "Saved test_max_50 split to data/processed/test_max_50.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save Processed Data\n",
    "# Save splits to parquet\n",
    "for split_name, split_df in [\n",
    "    ('train_max_50', train_df),\n",
    "    ('val_max_50', val_df),\n",
    "    ('test_max_50', test_df)\n",
    "]:\n",
    "    output_path = PROCESSED_DATA_DIR / f'{split_name}.parquet'\n",
    "    split_df.to_parquet(output_path, index=False)\n",
    "    print(f\"Saved {split_name} split to {output_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "environment_variables": {},
   "parameters": {
    "product": {
     "config": "/home/luuka/thesis/workspace/data/processed/preprocessing_config.json",
     "nb": "/home/luuka/thesis/workspace/output/01_preprocess.ipynb",
     "test": "/home/luuka/thesis/workspace/data/processed/test.parquet",
     "train": "/home/luuka/thesis/workspace/data/processed/train.parquet",
     "val": "/home/luuka/thesis/workspace/data/processed/val.parquet"
    }
   },
   "version": null
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
