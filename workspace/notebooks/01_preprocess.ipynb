{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "780d88a7",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# declare a list tasks whose products you want to use as inputs\n",
    "upstream = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5e452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from src.data_tools.preprocessor import load_and_clean_data\n",
    "from src.data_tools.dataset import create_data_splits\n",
    "from src.data_tools.preprocessor import save_data_to_json\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08b89778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved!\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "# Define paths\n",
    "DATA_DIR = Path('../data')\n",
    "PROCESSED_DATA_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model configuration\n",
    "config = {\n",
    "    'train_size': 0.7,\n",
    "    'val_size': 0.15,\n",
    "    'test_size': 0.15,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Save config\n",
    "with open(PROCESSED_DATA_DIR / 'preprocessing_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Configuration saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b39b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_tools.preprocessor:Loading Russian troll tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_tools.preprocessor:Loading Sentiment140 tweets...\n",
      "INFO:src.data_tools.preprocessor:Loading celebrity tweets...\n",
      "INFO:src.data_tools.preprocessor:Loading manualy scraped tweets...\n",
      "INFO:src.data_tools.preprocessor:Loading Twitter JSON data from non_troll_politics folder...\n",
      "INFO:src.data_tools.preprocessor:Loading information operations tweets...\n",
      "INFO:src.data_tools.preprocessor:Loading data collected by Machova...\n",
      "INFO:src.data_tools.preprocessor:Loading Civil Comments dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431b18f4ef624ba5878e272367dfc69d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85db841cfb8e49b59ecc516e9534ea8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00002.parquet:   0%|          | 0.00/194M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e30f012a854adf8663f88687576822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00002.parquet:   0%|          | 0.00/187M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8177941147e34ed5aa74cdbb24640483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62890c6a6414d26908cb2392210670b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1e0def819e414fbb067855f1141f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1804874 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae63b275390e4f11aa581b35724e4711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/97320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ccfd9837a44491b654fd7fe4085921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/97320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Czech comment data\n",
    "df_raw = load_and_clean_data(DATA_DIR)\n",
    "\n",
    "df_raw = df_raw.rename(columns={'account': 'author'})\n",
    "df_raw = df_raw.rename(columns={'tweet': 'text'})\n",
    "\n",
    "print(\"Raw dataset info:\")\n",
    "print(f\"Total samples: {len(df_raw)}\")\n",
    "print(f\"Columns: {df_raw.columns.tolist()}\")\n",
    "print(\"\\nMissing values:\")\n",
    "print(df_raw.isnull().sum())\n",
    "\n",
    "# Show class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df_raw['troll'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8b8142c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet distribution:\n",
      "Troll tweets: 3,731,059 (97.4%)\n",
      "Non-troll tweets: 99,754 (2.6%)\n",
      "Total tweets: 3,830,813\n"
     ]
    }
   ],
   "source": [
    "# Count tweets by troll label\n",
    "tweet_counts = df_raw['troll'].value_counts()\n",
    "total_tweets = len(df_raw)\n",
    "\n",
    "print(\"Tweet distribution:\")\n",
    "print(f\"Troll tweets: {tweet_counts[1]:,} ({tweet_counts[1]/total_tweets:.1%})\")\n",
    "print(f\"Non-troll tweets: {tweet_counts[0]:,} ({tweet_counts[0]/total_tweets:.1%})\")\n",
    "print(f\"Total tweets: {total_tweets:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "918d61fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m df_balanced \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m author \u001b[38;5;129;01min\u001b[39;00m troll_authors:\n\u001b[0;32m---> 21\u001b[0m     author_tweets \u001b[38;5;241m=\u001b[39m df_raw[\u001b[43mdf_raw\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauthor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mauthor\u001b[49m]\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(author_tweets) \u001b[38;5;241m>\u001b[39m max_tweets_per_author:\n\u001b[1;32m     23\u001b[0m         author_tweets \u001b[38;5;241m=\u001b[39m author_tweets\u001b[38;5;241m.\u001b[39msample(n\u001b[38;5;241m=\u001b[39mmax_tweets_per_author, random_state\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/pandas/core/series.py:6119\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6116\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   6117\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 6119\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:344\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 344\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:129\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    127\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# First balance authors as before\n",
    "author_labels = df_raw.groupby('author')['troll'].first()\n",
    "troll_authors = author_labels[author_labels == 1].index\n",
    "non_troll_authors = author_labels[author_labels == 0].index\n",
    "\n",
    "# Determine target size for authors\n",
    "target_author_size = min(len(troll_authors), len(non_troll_authors))\n",
    "\n",
    "# Sample authors\n",
    "np.random.seed(config['random_state'])\n",
    "if len(troll_authors) > target_author_size:\n",
    "    troll_authors = np.random.choice(troll_authors, size=target_author_size, replace=False)\n",
    "if len(non_troll_authors) > target_author_size:\n",
    "    non_troll_authors = np.random.choice(non_troll_authors, size=target_author_size, replace=False)\n",
    "\n",
    "# Now balance tweets per author\n",
    "max_tweets_per_author = 100  # Or whatever maximum you want to allow\n",
    "\n",
    "df_balanced = []\n",
    "for author in troll_authors:\n",
    "    author_tweets = df_raw[df_raw['author'] == author]\n",
    "    if len(author_tweets) > max_tweets_per_author:\n",
    "        author_tweets = author_tweets.sample(n=max_tweets_per_author, random_state=config['random_state'])\n",
    "    df_balanced.append(author_tweets)\n",
    "\n",
    "for author in non_troll_authors:\n",
    "    author_tweets = df_raw[df_raw['author'] == author]\n",
    "    if len(author_tweets) > max_tweets_per_author:\n",
    "        author_tweets = author_tweets.sample(n=max_tweets_per_author, random_state=config['random_state'])\n",
    "    df_balanced.append(author_tweets)\n",
    "\n",
    "df_balanced = pd.concat(df_balanced, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "928b8683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits:\n",
      "Train: 2977763 samples, 18167 authors\n",
      "Val:   574837 samples, 3893 authors\n",
      "Test:  857416 samples, 3893 authors\n"
     ]
    }
   ],
   "source": [
    "# Create Train/Val/Test Splits\n",
    "# Create splits ensuring no author overlap\n",
    "train_df, val_df, test_df = create_data_splits(\n",
    "    df_raw,\n",
    "    train_size=config['train_size'],\n",
    "    val_size=config['val_size'],\n",
    "    test_size=config['test_size'],\n",
    "    random_state=config['random_state']\n",
    ")\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "print(f\"Train: {len(train_df)} samples, {train_df['author'].nunique()} authors\")\n",
    "print(f\"Val:   {len(val_df)} samples, {val_df['author'].nunique()} authors\")\n",
    "print(f\"Test:  {len(test_df)} samples, {test_df['author'].nunique()} authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f05ff77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train split to ../data/processed/train.parquet\n",
      "Saved val split to ../data/processed/val.parquet\n",
      "Saved test split to ../data/processed/test.parquet\n",
      "\n",
      "Saved preprocessing config to ../data/processed/preprocessing_config.json\n"
     ]
    }
   ],
   "source": [
    "# Save Processed Data\n",
    "# Save splits to parquet\n",
    "for split_name, split_df in [\n",
    "    ('train', train_df),\n",
    "    ('val', val_df),\n",
    "    ('test', test_df)\n",
    "]:\n",
    "    output_path = PROCESSED_DATA_DIR / f'{split_name}.parquet'\n",
    "    split_df.to_parquet(output_path, index=False)\n",
    "    print(f\"Saved {split_name} split to {output_path}\")\n",
    "\n",
    "# Save preprocessing config\n",
    "import json\n",
    "config_path = PROCESSED_DATA_DIR / 'preprocessing_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f\"\\nSaved preprocessing config to {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b98d25b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "environment_variables": {},
   "parameters": {
    "product": {
     "config": "/home/luuka/thesis/workspace/data/processed/preprocessing_config.json",
     "nb": "/home/luuka/thesis/workspace/output/01_preprocess.ipynb",
     "test": "/home/luuka/thesis/workspace/data/processed/test.parquet",
     "train": "/home/luuka/thesis/workspace/data/processed/train.parquet",
     "val": "/home/luuka/thesis/workspace/data/processed/val.parquet"
    }
   },
   "version": null
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
