{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "780d88a7",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# declare a list tasks whose products you want to use as inputs\n",
    "upstream = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5e452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from src.data_tools.preprocessor import load_and_clean_data\n",
    "from src.data_tools.dataset import create_data_splits\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08b89778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved!\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "# Define paths\n",
    "DATA_DIR = Path('data')\n",
    "PROCESSED_DATA_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model configuration\n",
    "config = {\n",
    "    'min_text_length': 5,\n",
    "    'train_size': 0.7,\n",
    "    'val_size': 0.15,\n",
    "    'test_size': 0.15,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Save config\n",
    "with open(PROCESSED_DATA_DIR / 'preprocessing_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Configuration saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d38b39b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_tools.preprocessor:Loading Russian troll tweets...\n",
      "INFO:src.data_tools.preprocessor:Loading Sentiment140 tweets...\n",
      "INFO:src.data_tools.preprocessor:Loading celebrity tweets...\n",
      "INFO:src.data_tools.preprocessor:Loading manualy scraped tweets...\n",
      "INFO:src.data_tools.preprocessor:Loading Twitter JSON data from non_troll_politics folder...\n",
      "INFO:src.data_tools.preprocessor:Loading information operations tweets...\n",
      "INFO:src.data_tools.preprocessor:Loading data collected by Machova...\n",
      "INFO:src.data_tools.preprocessor:Non-troll data columns: ['is_troll', 'body']\n",
      "INFO:src.data_tools.preprocessor:Troll data columns: ['is_troll', 'body']\n",
      "INFO:src.data_tools.preprocessor:Combining datasets...\n",
      "INFO:src.data_tools.preprocessor:Filtering accounts with few tweets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset info:\n",
      "Total samples: 3510123\n",
      "Columns: ['author', 'text', 'troll']\n",
      "\n",
      "Missing values:\n",
      "author    0\n",
      "text      0\n",
      "troll     0\n",
      "dtype: int64\n",
      "\n",
      "Class distribution:\n",
      "troll\n",
      "1    0.83017\n",
      "0    0.16983\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load Czech comment data\n",
    "df_raw = load_and_clean_data(str(DATA_DIR))\n",
    "\n",
    "df_raw = df_raw.rename(columns={'account': 'author'})\n",
    "df_raw = df_raw.rename(columns={'tweet': 'text'})\n",
    "\n",
    "print(\"Raw dataset info:\")\n",
    "print(f\"Total samples: {len(df_raw)}\")\n",
    "print(f\"Columns: {df_raw.columns.tolist()}\")\n",
    "print(\"\\nMissing values:\")\n",
    "print(df_raw.isnull().sum())\n",
    "\n",
    "# Show class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df_raw['troll'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac6f873a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['author', 'text', 'troll']\n"
     ]
    }
   ],
   "source": [
    "print(\"Available columns:\", df_raw.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "928b8683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits:\n",
      "Train: 2407195 samples, 17033 authors\n",
      "Val:   541073 samples, 3650 authors\n",
      "Test:  561855 samples, 3651 authors\n"
     ]
    }
   ],
   "source": [
    "# Create Train/Val/Test Splits\n",
    "# Create splits ensuring no author overlap\n",
    "train_df, val_df, test_df = create_data_splits(\n",
    "    df_raw,\n",
    "    train_size=config['train_size'],\n",
    "    val_size=config['val_size'],\n",
    "    test_size=config['test_size'],\n",
    "    random_state=config['random_state']\n",
    ")\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "print(f\"Train: {len(train_df)} samples, {train_df['author'].nunique()} authors\")\n",
    "print(f\"Val:   {len(val_df)} samples, {val_df['author'].nunique()} authors\")\n",
    "print(f\"Test:  {len(test_df)} samples, {test_df['author'].nunique()} authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f05ff77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train split to ../data/processed/train.parquet\n",
      "Saved val split to ../data/processed/val.parquet\n",
      "Saved test split to ../data/processed/test.parquet\n",
      "\n",
      "Saved preprocessing config to ../data/processed/preprocessing_config.json\n"
     ]
    }
   ],
   "source": [
    "# Save Processed Data\n",
    "# Save splits to parquet\n",
    "for split_name, split_df in [\n",
    "    ('train', train_df),\n",
    "    ('val', val_df),\n",
    "    ('test', test_df)\n",
    "]:\n",
    "    output_path = PROCESSED_DATA_DIR / f'{split_name}.parquet'\n",
    "    split_df.to_parquet(output_path, index=False)\n",
    "    print(f\"Saved {split_name} split to {output_path}\")\n",
    "\n",
    "# Save preprocessing config\n",
    "import json\n",
    "config_path = PROCESSED_DATA_DIR / 'preprocessing_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f\"\\nSaved preprocessing config to {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f5abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DetectorFactory.seed = 42  # Ensure consistent detection\n",
    "\n",
    "def detect_language_safe(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Count comments per author and filter\n",
    "author_comment_counts = df_raw['author'].value_counts()\n",
    "authors_with_min_comments = author_comment_counts[author_comment_counts >= 5].index\n",
    "\n",
    "# Aggregate comments and detect language only for filtered authors\n",
    "print(\"Detecting language for authors with at least 5 comments...\")\n",
    "author_lang_df = df_raw[df_raw['author'].isin(authors_with_min_comments)] \\\n",
    "    .groupby('author')['text'].apply(lambda texts: ' '.join(texts)).reset_index()\n",
    "author_lang_df['lang'] = author_lang_df['text'].apply(detect_language_safe)\n",
    "\n",
    "# Merge back to original dataframe\n",
    "df = df_raw.merge(author_lang_df[['author', 'lang']], on='author', how='left')\n",
    "\n",
    "# Count and plot languages among filtered authors\n",
    "author_lang_counts = author_lang_df['lang'].value_counts()\n",
    "print(\"\\nLanguage Distribution (Authors with ≥ 5 comments):\")\n",
    "print(author_lang_counts)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=author_lang_counts.values, y=author_lang_counts.index)\n",
    "plt.xlabel('Number of Authors')\n",
    "plt.ylabel('Detected Language')\n",
    "plt.title('Detected Language of Authors (≥ 5 Comments)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print Slovak author count\n",
    "num_slovak_authors = (author_lang_df['lang'] == 'sk').sum()\n",
    "total_authors = len(author_lang_df)\n",
    "print(f\"\\nSlovak authors: {num_slovak_authors} / {total_authors} ({num_slovak_authors / total_authors:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc39c5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Filter Slovak authors (based on aggregate comment language)\n",
    "slovak_authors = author_lang_df[author_lang_df['lang'] == 'sk']['author'].tolist()\n",
    "\n",
    "# Define output path\n",
    "output_path = Path(\"./output/slovak_authors.json\")\n",
    "output_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "# Save as JSON\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(slovak_authors, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(slovak_authors)} Slovak authors to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "environment_variables": {},
   "parameters": {
    "product": {
     "config": "/home/luuka/thesis/workspace/data/processed/preprocessing_config.json",
     "nb": "/home/luuka/thesis/workspace/output/01_preprocess.ipynb",
     "test": "/home/luuka/thesis/workspace/data/processed/test.parquet",
     "train": "/home/luuka/thesis/workspace/data/processed/train.parquet",
     "val": "/home/luuka/thesis/workspace/data/processed/val.parquet"
    }
   },
   "version": null
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
