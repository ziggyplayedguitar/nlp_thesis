{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "780d88a7",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# declare a list tasks whose products you want to use as inputs\n",
    "upstream = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab5e452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from src.data_tools.preprocessor import load_and_clean_data\n",
    "from src.data_tools.dataset import create_data_splits\n",
    "from src.data_tools.preprocessor import save_data_to_json\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b89778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved!\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "# Define paths\n",
    "DATA_DIR = Path('data')\n",
    "PROCESSED_DATA_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model configuration\n",
    "config = {\n",
    "    'train_size': 0.7,\n",
    "    'val_size': 0.15,\n",
    "    'test_size': 0.15,\n",
    "    'random_state': 42\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d38b39b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_tools.preprocessor:Loading Russian troll tweets...\n",
      "INFO:src.data_tools.preprocessor:Loading Sentiment140 tweets...\n",
      "INFO:src.data_tools.preprocessor:Loading celebrity tweets...\n",
      "INFO:src.data_tools.preprocessor:Loading manualy scraped tweets...\n",
      "INFO:src.data_tools.preprocessor:Loading Twitter JSON data from non_troll_politics folder...\n",
      "INFO:src.data_tools.preprocessor:Loading information operations tweets...\n",
      "INFO:src.data_tools.preprocessor:Found 151 parquet files in information_operations folder and its subdirectories\n",
      "INFO:src.data_tools.preprocessor:Information operations data distribution - Trolls: 585379, Non-trolls: 914607\n",
      "INFO:src.data_tools.preprocessor:Loading data collected by Machova...\n",
      "INFO:src.data_tools.preprocessor:Loading Civil Comments dataset...\n",
      "INFO:src.data_tools.preprocessor:Combining datasets...\n",
      "INFO:src.data_tools.preprocessor:Filtering accounts with few tweets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset info:\n",
      "Total samples: 2999248\n",
      "Columns: ['author', 'text', 'troll', 'language']\n",
      "\n",
      "Missing values:\n",
      "author          0\n",
      "text            0\n",
      "troll           0\n",
      "language    15764\n",
      "dtype: int64\n",
      "\n",
      "Class distribution:\n",
      "troll\n",
      "0    0.785914\n",
      "1    0.214086\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load Czech comment data\n",
    "df_raw = load_and_clean_data(DATA_DIR, max_tweets_per_source=10000)\n",
    "\n",
    "df_raw = df_raw.rename(columns={'account': 'author'})\n",
    "df_raw = df_raw.rename(columns={'tweet': 'text'})\n",
    "\n",
    "print(\"Raw dataset info:\")\n",
    "print(f\"Total samples: {len(df_raw)}\")\n",
    "print(f\"Columns: {df_raw.columns.tolist()}\")\n",
    "print(\"\\nMissing values:\")\n",
    "print(df_raw.isnull().sum())\n",
    "\n",
    "# Show class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df_raw['troll'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8b8142c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet distribution:\n",
      "Troll tweets: 642,096 (21.4%)\n",
      "Non-troll tweets: 2,357,152 (78.6%)\n",
      "Total tweets: 2,999,248\n"
     ]
    }
   ],
   "source": [
    "# Count tweets by troll label\n",
    "tweet_counts = df_raw['troll'].value_counts()\n",
    "total_tweets = len(df_raw)\n",
    "\n",
    "print(\"Tweet distribution:\")\n",
    "print(f\"Troll tweets: {tweet_counts[1]:,} ({tweet_counts[1]/total_tweets:.1%})\")\n",
    "print(f\"Non-troll tweets: {tweet_counts[0]:,} ({tweet_counts[0]/total_tweets:.1%})\")\n",
    "print(f\"Total tweets: {total_tweets:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "918d61fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First balance authors\n",
    "# author_labels = df_raw.groupby('author')['troll'].first()\n",
    "# troll_authors = author_labels[author_labels == 1].index\n",
    "# non_troll_authors = author_labels[author_labels == 0].index\n",
    "\n",
    "# # Determine target size for authors\n",
    "# target_author_size = min(len(troll_authors), len(non_troll_authors))\n",
    "\n",
    "# # Sample authors\n",
    "# np.random.seed(config['random_state'])\n",
    "# if len(troll_authors) > target_author_size:\n",
    "#     troll_authors = np.random.choice(troll_authors, size=target_author_size, replace=False)\n",
    "# if len(non_troll_authors) > target_author_size:\n",
    "#     non_troll_authors = np.random.choice(non_troll_authors, size=target_author_size, replace=False)\n",
    "\n",
    "# # Now balance tweets per author\n",
    "# max_tweets_per_author = 100  # Or whatever maximum you want to allow\n",
    "\n",
    "# df_balanced = []\n",
    "# for author in troll_authors:\n",
    "#     author_tweets = df_raw[df_raw['author'] == author]\n",
    "#     if len(author_tweets) > max_tweets_per_author:\n",
    "#         author_tweets = author_tweets.sample(n=max_tweets_per_author, random_state=config['random_state'])\n",
    "#     df_balanced.append(author_tweets)\n",
    "\n",
    "# for author in non_troll_authors:\n",
    "#     author_tweets = df_raw[df_raw['author'] == author]\n",
    "#     if len(author_tweets) > max_tweets_per_author:\n",
    "#         author_tweets = author_tweets.sample(n=max_tweets_per_author, random_state=config['random_state'])\n",
    "#     df_balanced.append(author_tweets)\n",
    "\n",
    "# df_balanced = pd.concat(df_balanced, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c9816a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet distribution:\n",
      "Troll tweets: 220,731 (89.8%)\n",
      "Non-troll tweets: 25,169 (10.2%)\n",
      "Total tweets: 245,900\n"
     ]
    }
   ],
   "source": [
    "# # Count tweets by troll label\n",
    "# tweet_counts = df_balanced['troll'].value_counts()\n",
    "# total_tweets = len(df_balanced)\n",
    "\n",
    "# print(\"Tweet distribution:\")\n",
    "# print(f\"Troll tweets: {tweet_counts[1]:,} ({tweet_counts[1]/total_tweets:.1%})\")\n",
    "# print(f\"Non-troll tweets: {tweet_counts[0]:,} ({tweet_counts[0]/total_tweets:.1%})\")\n",
    "# print(f\"Total tweets: {total_tweets:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "928b8683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits:\n",
      "Train: 2002207 samples, 164903 authors\n",
      "Val:   457922 samples, 35336 authors\n",
      "Test:  539119 samples, 35337 authors\n"
     ]
    }
   ],
   "source": [
    "# Create Train/Val/Test Splits\n",
    "# Create splits ensuring no author overlap\n",
    "train_df, val_df, test_df = create_data_splits(\n",
    "    df_raw,\n",
    "    train_size=config['train_size'],\n",
    "    val_size=config['val_size'],\n",
    "    test_size=config['test_size'],\n",
    "    random_state=config['random_state']\n",
    ")\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "print(f\"Train: {len(train_df)} samples, {train_df['author'].nunique()} authors\")\n",
    "print(f\"Val:   {len(val_df)} samples, {val_df['author'].nunique()} authors\")\n",
    "print(f\"Test:  {len(test_df)} samples, {test_df['author'].nunique()} authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f05ff77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train split to data/processed/train.parquet\n",
      "Saved val split to data/processed/val.parquet\n",
      "Saved test split to data/processed/test.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save Processed Data\n",
    "# Save splits to parquet\n",
    "for split_name, split_df in [\n",
    "    ('train', train_df),\n",
    "    ('val', val_df),\n",
    "    ('test', test_df)\n",
    "]:\n",
    "    output_path = PROCESSED_DATA_DIR / f'{split_name}.parquet'\n",
    "    split_df.to_parquet(output_path, index=False)\n",
    "    print(f\"Saved {split_name} split to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b98d25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After resampling to 10% of authors:\n",
      "Train: 17838 samples, 1649 authors\n",
      "Val:   5044 samples, 353 authors\n",
      "Test:  3798 samples, 353 authors\n",
      "Saved resampled train split to data/processed/train_small.parquet\n",
      "Saved resampled val split to data/processed/val_small.parquet\n",
      "Saved resampled test split to data/processed/test_small.parquet\n"
     ]
    }
   ],
   "source": [
    "# Resample datasets by selecting 10% of authors and keeping all their posts\n",
    "train_authors = train_df['author'].unique()\n",
    "val_authors = val_df['author'].unique() \n",
    "test_authors = test_df['author'].unique()\n",
    "\n",
    "train_sampled_authors = np.random.choice(train_authors, size=int(len(train_authors)*0.1), replace=False)\n",
    "val_sampled_authors = np.random.choice(val_authors, size=int(len(val_authors)*0.1), replace=False)\n",
    "test_sampled_authors = np.random.choice(test_authors, size=int(len(test_authors)*0.1), replace=False)\n",
    "\n",
    "train_df = train_df[train_df['author'].isin(train_sampled_authors)]\n",
    "val_df = val_df[val_df['author'].isin(val_sampled_authors)]\n",
    "test_df = test_df[test_df['author'].isin(test_sampled_authors)]\n",
    "\n",
    "print(\"\\nAfter resampling to 10% of authors:\")\n",
    "print(f\"Train: {len(train_df)} samples, {train_df['author'].nunique()} authors\")\n",
    "print(f\"Val:   {len(val_df)} samples, {val_df['author'].nunique()} authors\") \n",
    "print(f\"Test:  {len(test_df)} samples, {test_df['author'].nunique()} authors\")\n",
    "\n",
    "# Save resampled splits\n",
    "for split_name, split_df in [\n",
    "    ('train', train_df),\n",
    "    ('val', val_df), \n",
    "    ('test', test_df)\n",
    "]:\n",
    "    output_path = PROCESSED_DATA_DIR / f'{split_name}_small.parquet'\n",
    "    split_df.to_parquet(output_path, index=False)\n",
    "    print(f\"Saved resampled {split_name} split to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f7929e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "environment_variables": {},
   "parameters": {
    "product": {
     "config": "/home/luuka/thesis/workspace/data/processed/preprocessing_config.json",
     "nb": "/home/luuka/thesis/workspace/output/01_preprocess.ipynb",
     "test": "/home/luuka/thesis/workspace/data/processed/test.parquet",
     "train": "/home/luuka/thesis/workspace/data/processed/train.parquet",
     "val": "/home/luuka/thesis/workspace/data/processed/val.parquet"
    }
   },
   "version": null
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
