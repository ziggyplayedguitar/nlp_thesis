{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MetaTrollDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(label)\n",
    "        }\n",
    "\n",
    "class MetaTrollClassifier(nn.Module):\n",
    "    def __init__(self, model_name='xlm-roberta-base', num_labels=2):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Linear(self.model.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_labels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        return self.adapter(pooled_output)\n",
    "\n",
    "class MetaTrollTrainer:\n",
    "    def __init__(self, model, device='cuda'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        \n",
    "    def create_episode(self, support_set, query_set, k_shot=5):\n",
    "        \"\"\"Create a few-shot learning episode\"\"\"\n",
    "        # Randomly sample k examples per class for support set\n",
    "        support_data = []\n",
    "        support_labels = []\n",
    "        for label in [0, 1]:  # Binary classification\n",
    "            indices = (support_set['labels'] == label).nonzero()[0]\n",
    "            selected = np.random.choice(indices, k_shot, replace=False)\n",
    "            support_data.extend(support_set['texts'][selected])\n",
    "            support_labels.extend([label] * k_shot)\n",
    "            \n",
    "        # Rest goes to query set\n",
    "        query_data = query_set['texts']\n",
    "        query_labels = query_set['labels']\n",
    "        \n",
    "        return {\n",
    "            'support': (support_data, support_labels),\n",
    "            'query': (query_data, query_labels)\n",
    "        }\n",
    "    \n",
    "    def train_episode(self, episode, optimizer, criterion):\n",
    "        self.model.train()\n",
    "        \n",
    "        # Train on support set\n",
    "        support_data, support_labels = episode['support']\n",
    "        support_dataset = MetaTrollDataset(support_data, support_labels, self.tokenizer)\n",
    "        support_loader = DataLoader(support_dataset, batch_size=len(support_dataset))\n",
    "        \n",
    "        for batch in support_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['label'].to(self.device)\n",
    "            \n",
    "            outputs = self.model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate on query set\n",
    "        self.model.eval()\n",
    "        query_data, query_labels = episode['query']\n",
    "        query_dataset = MetaTrollDataset(query_data, query_labels, self.tokenizer)\n",
    "        query_loader = DataLoader(query_dataset, batch_size=32)\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in query_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all comments data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████████████████████████████████████████████████| 124/124 [00:10<00:00, 11.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotations...\n",
      "Extracting comments for annotated users...\n",
      "Found 39 comments for user Štěpán Malák\n",
      "Found 64 comments for user Jan Benda\n",
      "Found 22 comments for user Jindra Macek\n",
      "Found 40 comments for user Josef Fortelný\n",
      "Found 37 comments for user Michal Musil\n",
      "Found 82 comments for user Pavel Rehberger\n",
      "Found 48 comments for user Vladimír Kalinay\n",
      "Found 42 comments for user Petr Jelinek\n",
      "Found 739 comments for user Jan Sykora\n",
      "Found 20 comments for user Radek Palán\n",
      "Found 64 comments for user Jan Trejbal\n",
      "Found 99 comments for user Michal Antonín\n",
      "Found 33 comments for user Gabi Muller\n",
      "Found 347 comments for user Michal Žák\n",
      "Found 25 comments for user Ivan Penzes\n",
      "Found 102 comments for user Richard Benes\n",
      "Found 55 comments for user Martin Ondík\n",
      "Found 71 comments for user Jan Kozohorský\n",
      "Found 21 comments for user Petr Mojžíš\n",
      "Found 44 comments for user Tomáš Souček\n",
      "Found 48 comments for user Libor Weizenbauer\n",
      "Found 196 comments for user Jan Velebil\n",
      "Found 60 comments for user Jakub Ručka\n",
      "Found 69 comments for user Jan Štěpán\n",
      "Found 28 comments for user Libor Pecháček\n",
      "Found 28 comments for user Jaroslav Obročník\n",
      "Found 97 comments for user Jaroslav Jirotka\n",
      "Found 61 comments for user Martin Kolařik\n",
      "Found 45 comments for user Jiří Karel\n",
      "Found 343 comments for user Pavel Safarik\n",
      "Found 49 comments for user Jirka Alex\n",
      "Found 133 comments for user Filip Kostka\n",
      "Found 38 comments for user Roman Blažej\n",
      "Found 78 comments for user Jirka Hlaváček\n",
      "\n",
      "Final dataset: 34 users with comments\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from src.data_tools.czech_data_tools import load_czech_media_data\n",
    "import pandas as pd\n",
    "\n",
    "# Load all comments data once\n",
    "print(\"Loading all comments data...\")\n",
    "all_comments_df = load_czech_media_data(\"./data/MediaSource\")\n",
    "\n",
    "# Load annotations\n",
    "print(\"Loading annotations...\")\n",
    "annotations_df = pd.read_csv('../workspace/annotations/user_labels.csv')\n",
    "certain_df = annotations_df[annotations_df['label'].isin([0, 1])]\n",
    "\n",
    "# Prepare data\n",
    "data = {\n",
    "    'texts': [],\n",
    "    'labels': []\n",
    "}\n",
    "\n",
    "# Extract comments for each annotated user from the already loaded dataframe\n",
    "print(\"Extracting comments for annotated users...\")\n",
    "for _, row in certain_df.iterrows():\n",
    "    user_comments = all_comments_df[all_comments_df['author'] == row['author']]['text'].tolist()\n",
    "    if user_comments:  # Only add if user has comments\n",
    "        data['texts'].append(' '.join(user_comments))\n",
    "        data['labels'].append(row['label'])\n",
    "        print(f\"Found {len(user_comments)} comments for user {row['author']}\")\n",
    "    else:\n",
    "        print(f\"No comments found for user {row['author']}\")\n",
    "\n",
    "print(f\"\\nFinal dataset: {len(data['texts'])} users with comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 34 examples\n",
      "Label distribution: [17 17]\n",
      "Episode 1, Accuracy: 0.5000\n",
      "Episode 2, Accuracy: 0.5000\n",
      "Episode 3, Accuracy: 0.5882\n",
      "Episode 4, Accuracy: 0.5294\n",
      "Episode 5, Accuracy: 0.5000\n",
      "Episode 6, Accuracy: 0.5000\n",
      "Episode 7, Accuracy: 0.5000\n",
      "Episode 8, Accuracy: 0.5000\n",
      "Episode 9, Accuracy: 0.5000\n",
      "Episode 10, Accuracy: 0.5000\n",
      "Episode 11, Accuracy: 0.5000\n",
      "Episode 12, Accuracy: 0.5000\n",
      "Episode 13, Accuracy: 0.5000\n",
      "Episode 14, Accuracy: 0.5000\n",
      "Episode 15, Accuracy: 0.5000\n",
      "Episode 16, Accuracy: 0.5000\n",
      "Episode 17, Accuracy: 0.5000\n",
      "Episode 18, Accuracy: 0.5000\n",
      "Episode 19, Accuracy: 0.5000\n",
      "Episode 20, Accuracy: 0.5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 34\u001b[0m\n\u001b[1;32m     27\u001b[0m episode_data \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mcreate_episode(\n\u001b[1;32m     28\u001b[0m     support_set\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexts\u001b[39m\u001b[38;5;124m'\u001b[39m: data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexts\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]},\n\u001b[1;32m     29\u001b[0m     query_set\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexts\u001b[39m\u001b[38;5;124m'\u001b[39m: data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexts\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]},\n\u001b[1;32m     30\u001b[0m     k_shot\u001b[38;5;241m=\u001b[39mk_shot\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Train on episode\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisode_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 106\u001b[0m, in \u001b[0;36mMetaTrollTrainer.train_episode\u001b[0;34m(self, episode, optimizer, criterion)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m query_loader:\n\u001b[0;32m--> 106\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    108\u001b[0m         labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# After loading and processing comments\n",
    "import numpy as np\n",
    "\n",
    "# Convert lists to numpy arrays for easier handling\n",
    "data = {\n",
    "    'texts': np.array(data['texts']),\n",
    "    'labels': np.array(data['labels'])\n",
    "}\n",
    "\n",
    "# Initialize model and trainer\n",
    "model = MetaTrollClassifier(model_name='xlm-roberta-base')\n",
    "trainer = MetaTrollTrainer(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "trainer.tokenizer = tokenizer  # Add tokenizer to trainer\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 100\n",
    "k_shot = 5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Starting training with {len(data['texts'])} examples\")\n",
    "print(f\"Label distribution: {np.bincount(data['labels'])}\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Create episode\n",
    "    episode_data = trainer.create_episode(\n",
    "        support_set={'texts': data['texts'], 'labels': data['labels']},\n",
    "        query_set={'texts': data['texts'], 'labels': data['labels']},\n",
    "        k_shot=k_shot\n",
    "    )\n",
    "    \n",
    "    # Train on episode\n",
    "    accuracy = trainer.train_episode(episode_data, optimizer, criterion)\n",
    "    print(f\"Episode {episode + 1}, Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
