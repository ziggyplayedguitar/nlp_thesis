{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f48636d5",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# declare a list tasks whose products you want to use as inputs\n",
    "upstream = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e267ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Import custom modules\n",
    "from src.models.bert_model import TrollDetector\n",
    "from src.models.trainer import TrollDetectorTrainer\n",
    "from src.data_tools.dataset import TrollDataset, collate_batch\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c92cb16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "model_name: distilbert-base-multilingual-cased\n",
      "max_length: 96\n",
      "batch_size: 32\n",
      "learning_rate: 2e-05\n",
      "weight_decay: 0.03\n",
      "num_epochs: 3\n",
      "dropout_rate: 0.2\n",
      "warmup_steps: 50\n",
      "max_grad_norm: 1.0\n",
      "comments_per_user: 10\n",
      "early_stopping_patience: 3\n",
      "use_wandb: False\n",
      "random_state: 42\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "DATA_DIR = Path('data')\n",
    "PROCESSED_DATA_DIR = DATA_DIR / 'processed'\n",
    "CHECKPOINT_DIR = Path('./checkpoints')\n",
    "\n",
    "# Create checkpoint directory\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Updated training configuration\n",
    "config = {\n",
    "    'model_name': 'distilbert-base-multilingual-cased',\n",
    "    'max_length': 96,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 2e-5,\n",
    "    'weight_decay': 0.03,\n",
    "    'num_epochs': 3,\n",
    "    'dropout_rate': 0.2,\n",
    "    'warmup_steps': 50,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'comments_per_user': 10,\n",
    "    'early_stopping_patience': 3,\n",
    "    'use_wandb': False,\n",
    "    'random_state': 17,\n",
    "}\n",
    "\n",
    "# Try to load preprocessing config\n",
    "try:\n",
    "    with open(PROCESSED_DATA_DIR / 'preprocessing_config.json', 'r') as f:\n",
    "        preproc_config = json.load(f)\n",
    "        config['random_state'] = preproc_config.get('random_state', 42)\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: preprocessing_config.json not found, using default random_state\")\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3767e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 17838 samples, 1649 authors\n",
      "Val:   5044 samples, 353 authors\n",
      "Test:  3798 samples, 353 authors\n"
     ]
    }
   ],
   "source": [
    "# # Load preprocessed data splits\n",
    "# train_df = pd.read_parquet(PROCESSED_DATA_DIR / 'train.parquet')\n",
    "# val_df = pd.read_parquet(PROCESSED_DATA_DIR / 'val.parquet')\n",
    "# test_df = pd.read_parquet(PROCESSED_DATA_DIR / 'test.parquet')\n",
    "\n",
    "# # Load preprocessed small data splits\n",
    "train_df = pd.read_parquet(PROCESSED_DATA_DIR / 'train_small.parquet')\n",
    "val_df = pd.read_parquet(PROCESSED_DATA_DIR / 'val_small.parquet')\n",
    "test_df = pd.read_parquet(PROCESSED_DATA_DIR / 'test_small.parquet')\n",
    "\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"Train: {len(train_df)} samples, {train_df['author'].nunique()} authors\")\n",
    "print(f\"Val:   {len(val_df)} samples, {val_df['author'].nunique()} authors\")\n",
    "print(f\"Test:  {len(test_df)} samples, {test_df['author'].nunique()} authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fe3bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_tools.dataset:Using 'text' as text column\n",
      "INFO:src.data_tools.dataset:Labels are already normalized between 0 and 1\n",
      "INFO:src.data_tools.dataset:Created 2101 samples from 1649 authors\n",
      "INFO:src.data_tools.dataset:Using 'text' as text column\n",
      "INFO:src.data_tools.dataset:Labels are already normalized between 0 and 1\n",
      "INFO:src.data_tools.dataset:Created 559 samples from 353 authors\n",
      "INFO:src.data_tools.dataset:Using 'text' as text column\n",
      "INFO:src.data_tools.dataset:Labels are already normalized between 0 and 1\n",
      "INFO:src.data_tools.dataset:Created 439 samples from 353 authors\n"
     ]
    }
   ],
   "source": [
    "# Create Datasets with regression settings\n",
    "train_dataset = TrollDataset(\n",
    "    train_df,\n",
    "    tokenizer_name=config['model_name'],\n",
    "    max_length=config['max_length'],\n",
    "    comments_per_user=config['comments_per_user'],\n",
    "    label_column='troll',  # or your trolliness score column\n",
    "    normalize_labels=True  # This will automatically normalize scores to [0,1]\n",
    ")\n",
    "\n",
    "val_dataset = TrollDataset(\n",
    "    val_df,\n",
    "    tokenizer_name=config['model_name'],\n",
    "    max_length=config['max_length'],\n",
    "    comments_per_user=config['comments_per_user'],\n",
    "    label_column='troll',\n",
    "    normalize_labels=True\n",
    ")\n",
    "\n",
    "test_dataset = TrollDataset(\n",
    "    test_df,\n",
    "    tokenizer_name=config['model_name'],\n",
    "    max_length=config['max_length'],\n",
    "    comments_per_user=config['comments_per_user'],\n",
    "    label_column='troll',\n",
    "    normalize_labels=True\n",
    ")\n",
    "\n",
    "# Create dataloaders (unchanged)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be1b89ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luuka/thesis/workspace/src/models/trainer.py:49: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model and Trainer\n",
    "model = TrollDetector(\n",
    "    model_name=config['model_name'],\n",
    "    dropout_rate=config['dropout_rate']\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = TrollDetectorTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    learning_rate=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay'],\n",
    "    max_grad_norm=config['max_grad_norm'],\n",
    "    num_epochs=config['num_epochs'],\n",
    "    warmup_steps=config['warmup_steps'],\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    use_wandb=config['use_wandb']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d20b70ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimport trainer after changes\n",
    "from importlib import reload\n",
    "from src.models.trainer import TrollDetectorTrainer\n",
    "reload(sys.modules['src.models.trainer'])\n",
    "from src.models.trainer import TrollDetectorTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e34eaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.models.trainer:Starting training on device: cuda\n",
      "INFO:src.models.trainer:Training samples: 2101\n",
      "INFO:src.models.trainer:Validation samples: 559\n",
      "INFO:src.models.trainer:\n",
      "Epoch 1/3\n",
      "Training: 100%|████████████████████████████████████████████████████████████| 66/66 [01:45<00:00,  1.60s/it, loss=0.0357]\n",
      "INFO:src.models.trainer:Training metrics: {'mse': 0.15188526519692985, 'rmse': 0.38972460173426293, 'mae': 0.35933035012059567, 'r2': -1.051097793574841, 'binary_accuracy': 0.8389261744966443, 'loss': 0.07453820440974651, 'num_authors': 298}\n",
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████████| 18/18 [00:32<00:00,  1.79s/it]\n",
      "INFO:src.models.trainer:Validation metrics: {'mse': 0.11774399761962906, 'rmse': 0.3431384525517784, 'mae': 0.3059161580167711, 'r2': -0.07651654966518007, 'binary_accuracy': 0.875, 'loss': 0.05636216477594442, 'num_authors': 80}\n",
      "INFO:src.models.trainer:New best model with validation R²: -0.0765\n",
      "INFO:src.models.trainer:\n",
      "Epoch 2/3\n",
      "Training:   0%|                                                                                  | 0/66 [00:00<?, ?it/s]/home/luuka/thesis/workspace/src/models/trainer.py:93: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|████████████████████████████████████████████████████████████| 66/66 [01:47<00:00,  1.63s/it, loss=0.0231]\n",
      "INFO:src.models.trainer:Training metrics: {'mse': 0.1183889915138458, 'rmse': 0.34407701392834394, 'mae': 0.2683087035313549, 'r2': -0.5987554748168431, 'binary_accuracy': 0.7986577181208053, 'loss': 0.025388877062747877, 'num_authors': 298}\n",
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████████| 18/18 [00:30<00:00,  1.68s/it]\n",
      "INFO:src.models.trainer:Validation metrics: {'mse': 0.13311222985151983, 'rmse': 0.3648454876403432, 'mae': 0.25825287876650693, 'r2': -0.21702610149960977, 'binary_accuracy': 0.7625, 'loss': 0.03130349509754322, 'num_authors': 80}\n",
      "INFO:src.models.trainer:\n",
      "Epoch 3/3\n",
      "Training:   0%|                                                                                  | 0/66 [00:00<?, ?it/s]/home/luuka/thesis/workspace/src/models/trainer.py:93: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|████████████████████████████████████████████████████████████| 66/66 [01:45<00:00,  1.60s/it, loss=0.0343]\n",
      "INFO:src.models.trainer:Training metrics: {'mse': 0.07284920079353885, 'rmse': 0.26990591100148004, 'mae': 0.17553582927524644, 'r2': 0.01622560412569607, 'binary_accuracy': 0.8993288590604027, 'loss': 0.012870465751003587, 'num_authors': 298}\n",
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████████| 18/18 [00:33<00:00,  1.86s/it]\n",
      "INFO:src.models.trainer:Validation metrics: {'mse': 0.07385331073267523, 'rmse': 0.2717596561903095, 'mae': 0.16346366300713272, 'r2': 0.32476973044411217, 'binary_accuracy': 0.9, 'loss': 0.029750166316969424, 'num_authors': 80}\n",
      "INFO:src.models.trainer:New best model with validation R²: 0.3248\n",
      "INFO:src.models.trainer:\n",
      "Evaluating on test set...\n",
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████████| 14/14 [00:22<00:00,  1.61s/it]\n",
      "INFO:src.models.trainer:Test metrics: {'mse': 0.1471374701433877, 'rmse': 0.3835850233564753, 'mae': 0.2528228883712012, 'r2': -0.25372589819580105, 'binary_accuracy': 0.7654320987654321, 'loss': 0.0421686741974554, 'num_authors': 81}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n",
      "\n",
      "Final metrics:\n",
      "mse: 0.1471\n",
      "rmse: 0.3836\n",
      "mae: 0.2528\n",
      "r2: -0.2537\n",
      "binary_accuracy: 0.7654\n",
      "loss: 0.0422\n",
      "num_authors: 81.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "final_metrics = trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(\"\\nFinal metrics:\")\n",
    "for metric, value in final_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Save final configuration and results\n",
    "results = {\n",
    "    'config': config,\n",
    "    'final_metrics': final_metrics\n",
    "}\n",
    "\n",
    "with open(CHECKPOINT_DIR / 'training_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b69dc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved final model to: checkpoints/best_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Save the final model state\n",
    "final_model_path = CHECKPOINT_DIR / 'best_model.pt'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"\\nSaved final model to: {final_model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "environment_variables": {},
   "parameters": {
    "product": {
     "metrics": "/home/luuka/thesis/workspace/checkpoints/best_model_info.json",
     "model": "/home/luuka/thesis/workspace/checkpoints/best_model.pt",
     "nb": "/home/luuka/thesis/workspace/output/02_train.ipynb",
     "results": "/home/luuka/thesis/workspace/checkpoints/training_results.json"
    }
   },
   "version": null
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
