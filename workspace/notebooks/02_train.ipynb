{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f48636d5",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# declare a list tasks whose products you want to use as inputs\n",
    "upstream = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e267ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Import custom modules\n",
    "from src.models.bert_model import TrollDetector\n",
    "from src.models.trainer import TrollDetectorTrainer\n",
    "from src.data_tools.dataset import TrollDataset, collate_batch\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c92cb16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "model_name: distilbert-base-multilingual-cased\n",
      "adapter_path: None\n",
      "max_length: 96\n",
      "batch_size: 8\n",
      "learning_rate: 2e-05\n",
      "weight_decay: 0.03\n",
      "num_epochs: 3\n",
      "dropout_rate: 0.1\n",
      "warmup_steps: 50\n",
      "max_grad_norm: 1.0\n",
      "comments_per_user: 10\n",
      "early_stopping_patience: 3\n",
      "random_state: 42\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "DATA_DIR = Path('data')\n",
    "PROCESSED_DATA_DIR = DATA_DIR / 'processed'\n",
    "CHECKPOINT_DIR = Path('./checkpoints')\n",
    "\n",
    "# Create checkpoint directory\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Updated training configuration\n",
    "config = {\n",
    "    'model_name': 'distilbert-base-multilingual-cased',\n",
    "    'adapter_path': None, #Dont use adapter for first training\n",
    "    # 'model_name': 'ufal/robeczech-base',\n",
    "    'max_length': 96,\n",
    "    'batch_size': 8,\n",
    "    'learning_rate': 2e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'num_epochs': 3,\n",
    "    'dropout_rate': 0.2,\n",
    "    'warmup_steps': 50,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'comments_per_user': 10,\n",
    "    'early_stopping_patience': 3,\n",
    "    'random_state': 17,\n",
    "}\n",
    "\n",
    "# Try to load preprocessing config\n",
    "try:\n",
    "    with open(PROCESSED_DATA_DIR / 'preprocessing_config.json', 'r') as f:\n",
    "        preproc_config = json.load(f)\n",
    "        config['random_state'] = preproc_config.get('random_state', 42)\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: preprocessing_config.json not found, using default random_state\")\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dff16e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/absolute/path/to/LIWC2007_English100131.dic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mliwc\u001b[39;00m\n",
      "\u001b[0;32m----> 2\u001b[0m parse, category_names \u001b[38;5;241m=\u001b[39m \u001b[43mliwc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_token_parser\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/absolute/path/to/LIWC2007_English100131.dic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# or LIWC-2015, etc.\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/liwc/__init__.py:21\u001b[0m, in \u001b[0;36mload_token_parser\u001b[0;34m(filepath)\u001b[0m\n",
      "\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_token_parser\u001b[39m(filepath):\n",
      "\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    Reads a LIWC lexicon from a file in the .dic format, returning a tuple of\u001b[39;00m\n",
      "\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    (parse, category_names), where:\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m      the lexicon\u001b[39;00m\n",
      "\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m---> 21\u001b[0m     lexicon, category_names \u001b[38;5;241m=\u001b[39m \u001b[43mread_dic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     22\u001b[0m     trie \u001b[38;5;241m=\u001b[39m build_trie(lexicon)\n",
      "\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse_token\u001b[39m(token):\n",
      "\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/liwc/dic.py:36\u001b[0m, in \u001b[0;36mread_dic\u001b[0;34m(filepath)\u001b[0m\n",
      "\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_dic\u001b[39m(filepath):\n",
      "\u001b[1;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    Reads a LIWC lexicon from a file in the .dic format, returning a tuple of\u001b[39;00m\n",
      "\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    (lexicon, category_names), where:\u001b[39;00m\n",
      "\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    * `lexicon` is a dict mapping string patterns to lists of category names\u001b[39;00m\n",
      "\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    * `category_names` is a list of category names (as strings)\u001b[39;00m\n",
      "\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m lines:\n",
      "\u001b[1;32m     37\u001b[0m         \u001b[38;5;66;03m# read up to first \"%\" (should be very first line of file)\u001b[39;00m\n",
      "\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n",
      "\u001b[1;32m     39\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/absolute/path/to/LIWC2007_English100131.dic'"
     ]
    }
   ],
   "source": [
    "import liwc\n",
    "parse, category_names = liwc.load_token_parser(\n",
    "        \"/absolute/path/to/LIWC2007_English100131.dic\")  # or LIWC-2015, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3767e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 625987 samples, 8953 authors\n",
      "Val:   169654 samples, 1919 authors\n",
      "Test:  102276 samples, 1919 authors\n"
     ]
    }
   ],
   "source": [
    "# # Load preprocessed data splits\n",
    "# train_df = pd.read_parquet(PROCESSED_DATA_DIR / 'train.parquet')\n",
    "# val_df = pd.read_parquet(PROCESSED_DATA_DIR / 'val.parquet')\n",
    "# test_df = pd.read_parquet(PROCESSED_DATA_DIR / 'test.parquet')\n",
    "\n",
    "# # Load preprocessed small data splits\n",
    "train_df = pd.read_parquet(PROCESSED_DATA_DIR / 'train.parquet')\n",
    "val_df = pd.read_parquet(PROCESSED_DATA_DIR / 'val.parquet')\n",
    "test_df = pd.read_parquet(PROCESSED_DATA_DIR / 'test.parquet')\n",
    "\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"Train: {len(train_df)} samples, {train_df['author'].nunique()} authors\")\n",
    "print(f\"Val:   {len(val_df)} samples, {val_df['author'].nunique()} authors\")\n",
    "print(f\"Test:  {len(test_df)} samples, {test_df['author'].nunique()} authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c540e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset sizes after filtering for English:\n",
      "Train: 346079 samples, 6007 authors\n",
      "Val:   117871 samples, 1314 authors\n",
      "Test:  48815 samples, 1285 authors\n"
     ]
    }
   ],
   "source": [
    "# Filter datasets to only include English content\n",
    "train_df = train_df[train_df['language'].isin(['en', 'English'])]\n",
    "val_df = val_df[val_df['language'].isin(['en', 'English'])]\n",
    "test_df = test_df[test_df['language'].isin(['en', 'English'])]\n",
    "\n",
    "print(\"\\nDataset sizes after filtering for English:\")\n",
    "print(f\"Train: {len(train_df)} samples, {train_df['author'].nunique()} authors\")\n",
    "print(f\"Val:   {len(val_df)} samples, {val_df['author'].nunique()} authors\") \n",
    "print(f\"Test:  {len(test_df)} samples, {test_df['author'].nunique()} authors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45fe3bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_tools.dataset:Using 'text' as text column\n",
      "INFO:src.data_tools.dataset:Labels are already normalized between 0 and 1\n",
      "INFO:src.data_tools.dataset:Created 19684 samples from 6007 authors\n",
      "INFO:src.data_tools.dataset:Using 'text' as text column\n",
      "INFO:src.data_tools.dataset:Labels are already normalized between 0 and 1\n",
      "INFO:src.data_tools.dataset:Created 4752 samples from 1314 authors\n",
      "INFO:src.data_tools.dataset:Using 'text' as text column\n",
      "INFO:src.data_tools.dataset:Labels are already normalized between 0 and 1\n",
      "INFO:src.data_tools.dataset:Created 4276 samples from 1285 authors\n"
     ]
    }
   ],
   "source": [
    "# Create Datasets with regression settings\n",
    "train_dataset = TrollDataset(\n",
    "    train_df,\n",
    "    tokenizer_name=config['model_name'],\n",
    "    max_length=config['max_length'],\n",
    "    comments_per_user=config['comments_per_user'],\n",
    "    label_column='troll',  # or your trolliness score column\n",
    "    normalize_labels=True  # This will automatically normalize scores to [0,1]\n",
    ")\n",
    "\n",
    "val_dataset = TrollDataset(\n",
    "    val_df,\n",
    "    tokenizer_name=config['model_name'],\n",
    "    max_length=config['max_length'],\n",
    "    comments_per_user=config['comments_per_user'],\n",
    "    label_column='troll',\n",
    "    normalize_labels=True\n",
    ")\n",
    "\n",
    "test_dataset = TrollDataset(\n",
    "    test_df,\n",
    "    tokenizer_name=config['model_name'],\n",
    "    max_length=config['max_length'],\n",
    "    comments_per_user=config['comments_per_user'],\n",
    "    label_column='troll',\n",
    "    normalize_labels=True\n",
    ")\n",
    "\n",
    "# Create dataloaders (unchanged)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be1b89ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:adapters.heads.model_mixin:Adding head 'default' with config {'head_type': 'masked_lm', 'vocab_size': 119547, 'embedding_size': 768, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
      "/home/luuka/thesis/workspace/src/models/trainer.py:49: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model and Trainer\n",
    "model = TrollDetector(\n",
    "    model_name=config['model_name'],\n",
    "    adapter_path=config['adapter_path'],\n",
    "    dropout_rate=config['dropout_rate']\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = TrollDetectorTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    learning_rate=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay'],\n",
    "    max_grad_norm=config['max_grad_norm'],\n",
    "    num_epochs=config['num_epochs'],\n",
    "    warmup_steps=config['warmup_steps'],\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d07369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameter tensors: 111\n"
     ]
    }
   ],
   "source": [
    "# 1 – right after you construct TrollDetector\n",
    "trainable = [n for n, p in model.named_parameters() if p.requires_grad]\n",
    "print(\"Trainable parameter tensors:\", len(trainable))\n",
    "assert len(trainable) > 0, \"Nothing to train!\"\n",
    "\n",
    "# # 2 – right after you build the optimiser\n",
    "# print(\"Optimizer param groups:\",\n",
    "#       sum(p.numel() for p in optimizer.param_groups[0]['params']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e34eaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.models.trainer:Starting training on device: cuda\n",
      "INFO:src.models.trainer:Training samples: 10790\n",
      "INFO:src.models.trainer:Validation samples: 2383\n",
      "INFO:src.models.trainer:\n",
      "Epoch 1/3\n",
      "Training:   0%|                                                                                | 0/1349 [00:00<?, ?it/s]/home/luuka/thesis/workspace/src/models/trainer.py:93: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|████████████████████████████████████████████████████████| 1349/1349 [07:01<00:00,  3.20it/s, loss=0.0879]\n",
      "INFO:src.models.trainer:Training metrics: {'mse': 0.038329243539302865, 'rmse': 0.19577855740428488, 'mae': 0.09667715059744345, 'r2': 0.8239332464107989, 'binary_accuracy': 0.9508445945945946, 'loss': 0.017497718319414157, 'num_authors': 5920}\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████| 298/298 [02:07<00:00,  2.34it/s]\n",
      "INFO:src.models.trainer:Validation metrics: {'mse': 0.08737124002524761, 'rmse': 0.2955862649468808, 'mae': 0.15593152526600362, 'r2': 0.5936431075659179, 'binary_accuracy': 0.8872120730738682, 'loss': 0.04368227651320922, 'num_authors': 1259}\n",
      "INFO:src.models.trainer:\n",
      "Epoch 2/3\n",
      "Training:   0%|                                                                                | 0/1349 [00:00<?, ?it/s]/home/luuka/thesis/workspace/src/models/trainer.py:93: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|████████████████████████████████████████████████████████| 1349/1349 [04:53<00:00,  4.60it/s, loss=0.0007]\n",
      "INFO:src.models.trainer:Training metrics: {'mse': 0.038239355738064536, 'rmse': 0.19554885767517163, 'mae': 0.09592955434644544, 'r2': 0.8243461492465952, 'binary_accuracy': 0.9491554054054054, 'loss': 0.017286120221617306, 'num_authors': 5920}\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████| 298/298 [01:38<00:00,  3.03it/s]\n",
      "INFO:src.models.trainer:Validation metrics: {'mse': 0.08737124002524761, 'rmse': 0.2955862649468808, 'mae': 0.15593152526600362, 'r2': 0.5936431075659179, 'binary_accuracy': 0.8872120730738682, 'loss': 0.04368227651320922, 'num_authors': 1259}\n",
      "INFO:src.models.trainer:\n",
      "Epoch 3/3\n",
      "Training:   0%|                                                                                | 0/1349 [00:00<?, ?it/s]/home/luuka/thesis/workspace/src/models/trainer.py:93: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|████████████████████████████████████████████████████████| 1349/1349 [04:20<00:00,  5.18it/s, loss=0.0808]\n",
      "INFO:src.models.trainer:Training metrics: {'mse': 0.038570256608828284, 'rmse': 0.1963931175189912, 'mae': 0.09612741663649275, 'r2': 0.8228261442400921, 'binary_accuracy': 0.9476351351351351, 'loss': 0.0172147113842785, 'num_authors': 5920}\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████| 298/298 [01:44<00:00,  2.86it/s]\n",
      "INFO:src.models.trainer:Validation metrics: {'mse': 0.08737124002524761, 'rmse': 0.2955862649468808, 'mae': 0.15593152526600362, 'r2': 0.5936431075659179, 'binary_accuracy': 0.8872120730738682, 'loss': 0.04368227651320922, 'num_authors': 1259}\n",
      "INFO:src.models.trainer:\n",
      "Evaluating on test set...\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████| 288/288 [01:50<00:00,  2.62it/s]\n",
      "INFO:src.models.trainer:Test metrics: {'mse': 0.09499449190119336, 'rmse': 0.3082117647027663, 'mae': 0.16288919353847103, 'r2': 0.552132588761497, 'binary_accuracy': 0.8804523424878837, 'loss': 0.041311888827673705, 'num_authors': 1238}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n",
      "\n",
      "Final metrics:\n",
      "mse: 0.0950\n",
      "rmse: 0.3082\n",
      "mae: 0.1629\n",
      "r2: 0.5521\n",
      "binary_accuracy: 0.8805\n",
      "loss: 0.0413\n",
      "num_authors: 1238.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "final_metrics = trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(\"\\nFinal metrics:\")\n",
    "for metric, value in final_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Save final configuration and results\n",
    "results = {\n",
    "    'config': config,\n",
    "    'final_metrics': final_metrics\n",
    "}\n",
    "\n",
    "with open(CHECKPOINT_DIR / 'training_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b69dc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved final model to: checkpoints/best_model_english_small.pt\n"
     ]
    }
   ],
   "source": [
    "# Save the final model state\n",
    "final_model_path = CHECKPOINT_DIR / 'best_model_english_small.pt'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"\\nSaved final model to: {final_model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "environment_variables": {},
   "parameters": {
    "product": {
     "metrics": "/home/luuka/thesis/workspace/checkpoints/best_model_info.json",
     "model": "/home/luuka/thesis/workspace/checkpoints/best_model.pt",
     "nb": "/home/luuka/thesis/workspace/output/02_train.ipynb",
     "results": "/home/luuka/thesis/workspace/checkpoints/training_results.json"
    }
   },
   "version": null
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
