{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f48636d5",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# declare a list tasks whose products you want to use as inputs\n",
    "upstream = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e267ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Import custom modules\n",
    "from src.models.bert_model import TrollDetector\n",
    "from src.models.trainer import TrollDetectorTrainer\n",
    "from src.data_tools.dataset import TrollDataset, collate_batch\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92cb16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "model_name: distilbert-base-multilingual-cased\n",
      "max_length: 96\n",
      "batch_size: 32\n",
      "learning_rate: 2e-05\n",
      "weight_decay: 0.03\n",
      "num_epochs: 3\n",
      "dropout_rate: 0.2\n",
      "warmup_steps: 50\n",
      "max_grad_norm: 1.0\n",
      "comments_per_user: 10\n",
      "early_stopping_patience: 3\n",
      "use_wandb: False\n",
      "random_state: 42\n",
      "label_smoothing: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "DATA_DIR = Path('../data')\n",
    "PROCESSED_DATA_DIR = DATA_DIR / 'processed'\n",
    "CHECKPOINT_DIR = Path('../checkpoints')\n",
    "\n",
    "# Create checkpoint directory\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training configuration\n",
    "config = {\n",
    "    'model_name': 'distilbert-base-multilingual-cased',\n",
    "    'max_length': 96,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 2e-5,\n",
    "    'weight_decay': 0.03,\n",
    "    'num_epochs': 3,\n",
    "    'dropout_rate': 0.2,\n",
    "    'warmup_steps': 50,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'comments_per_user': 10,\n",
    "    'early_stopping_patience': 3,\n",
    "    'use_wandb': False,\n",
    "    'random_state': 17,  # Default if config not found\n",
    "    'label_smoothing': 0.1        # Added label smoothing\n",
    "}\n",
    "\n",
    "# Try to load preprocessing config\n",
    "try:\n",
    "    with open(PROCESSED_DATA_DIR / 'preprocessing_config.json', 'r') as f:\n",
    "        preproc_config = json.load(f)\n",
    "        config['random_state'] = preproc_config.get('random_state', 42)\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: preprocessing_config.json not found, using default random_state\")\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3767e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 171242 samples, 4188 authors\n",
      "Val:   39523 samples, 898 authors\n",
      "Test:  35135 samples, 898 authors\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data splits\n",
    "train_df = pd.read_parquet(PROCESSED_DATA_DIR / 'train.parquet')\n",
    "val_df = pd.read_parquet(PROCESSED_DATA_DIR / 'val.parquet')\n",
    "test_df = pd.read_parquet(PROCESSED_DATA_DIR / 'test.parquet')\n",
    "\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"Train: {len(train_df)} samples, {train_df['author'].nunique()} authors\")\n",
    "print(f\"Val:   {len(val_df)} samples, {val_df['author'].nunique()} authors\")\n",
    "print(f\"Test:  {len(test_df)} samples, {test_df['author'].nunique()} authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45fe3bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_tools.dataset:Using 'text' as text column\n",
      "INFO:src.data_tools.dataset:Created 17356 samples from 4188 authors\n",
      "INFO:src.data_tools.dataset:Using 'text' as text column\n",
      "INFO:src.data_tools.dataset:Created 4019 samples from 898 authors\n",
      "INFO:src.data_tools.dataset:Using 'text' as text column\n",
      "INFO:src.data_tools.dataset:Created 3572 samples from 898 authors\n"
     ]
    }
   ],
   "source": [
    "# Create Datasets and DataLoaders\n",
    "# Initialize datasets\n",
    "train_dataset = TrollDataset(\n",
    "    train_df,\n",
    "    tokenizer_name=config['model_name'],\n",
    "    max_length=config['max_length'],\n",
    "    comments_per_user=config['comments_per_user']\n",
    ")\n",
    "\n",
    "val_dataset = TrollDataset(\n",
    "    val_df,\n",
    "    tokenizer_name=config['model_name'],\n",
    "    max_length=config['max_length'],\n",
    "    comments_per_user=config['comments_per_user']\n",
    ")\n",
    "\n",
    "test_dataset = TrollDataset(\n",
    "    test_df,\n",
    "    tokenizer_name=config['model_name'],\n",
    "    max_length=config['max_length'],\n",
    "    comments_per_user=config['comments_per_user']\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be1b89ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luuka/thesis/workspace/src/models/trainer.py:49: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model and Trainer\n",
    "model = TrollDetector(\n",
    "    model_name=config['model_name'],\n",
    "    dropout_rate=config['dropout_rate']\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = TrollDetectorTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    learning_rate=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay'],\n",
    "    max_grad_norm=config['max_grad_norm'],\n",
    "    num_epochs=config['num_epochs'],\n",
    "    warmup_steps=config['warmup_steps'],\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    use_wandb=config['use_wandb']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e34eaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.models.trainer:Starting training on device: cuda\n",
      "INFO:src.models.trainer:Training samples: 17356\n",
      "INFO:src.models.trainer:Validation samples: 4019\n",
      "INFO:src.models.trainer:\n",
      "Epoch 1/3\n",
      "Training:   0%|          | 0/543 [00:00<?, ?it/s]/home/luuka/thesis/workspace/src/models/trainer.py:93: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Training: 100%|██████████| 543/543 [12:11<00:00,  1.35s/it, loss=0.0079]\n",
      "INFO:src.models.trainer:Training metrics: {'accuracy': 0.9331295376385174, 'f1': 0.9296849498081569, 'precision': 0.9340245949837218, 'recall': 0.9331295376385174, 'loss': 0.10678839946464115, 'num_authors': 2617}\n",
      "Evaluating: 100%|██████████| 126/126 [03:03<00:00,  1.46s/it]\n",
      "INFO:src.models.trainer:Evaluated 586 unique authors, 413 with multiple batches\n",
      "INFO:src.models.trainer:Average batches per author: 6.86\n",
      "INFO:src.models.trainer:Validation metrics: {'accuracy': 0.9607508532423208, 'f1': 0.959823624364068, 'precision': 0.9608523198966884, 'recall': 0.9607508532423208, 'auc': 0.9917184265010353, 'loss': 0.05787931462114174, 'num_authors': 586}\n",
      "INFO:src.models.trainer:New best model with validation AUC: 0.9917\n",
      "INFO:src.models.trainer:\n",
      "Epoch 2/3\n",
      "Training:   0%|          | 0/543 [00:00<?, ?it/s]/home/luuka/thesis/workspace/src/models/trainer.py:93: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Training: 100%|██████████| 543/543 [13:10<00:00,  1.46s/it, loss=0.0020]\n",
      "INFO:src.models.trainer:Training metrics: {'accuracy': 0.9793656858998854, 'f1': 0.9791536082453364, 'precision': 0.9793338097704825, 'recall': 0.9793656858998854, 'loss': 0.03741646712623591, 'num_authors': 2617}\n",
      "Evaluating: 100%|██████████| 126/126 [02:59<00:00,  1.42s/it]\n",
      "INFO:src.models.trainer:Evaluated 586 unique authors, 413 with multiple batches\n",
      "INFO:src.models.trainer:Average batches per author: 6.86\n",
      "INFO:src.models.trainer:Validation metrics: {'accuracy': 0.962457337883959, 'f1': 0.9615055827238392, 'precision': 0.962802545777852, 'recall': 0.962457337883959, 'auc': 0.9886818495514148, 'loss': 0.06435404111915785, 'num_authors': 586}\n",
      "INFO:src.models.trainer:\n",
      "Epoch 3/3\n",
      "Training:   0%|          | 0/543 [00:00<?, ?it/s]/home/luuka/thesis/workspace/src/models/trainer.py:93: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Training: 100%|██████████| 543/543 [11:16<00:00,  1.25s/it, loss=0.0028]\n",
      "INFO:src.models.trainer:Training metrics: {'accuracy': 0.9896828429499427, 'f1': 0.9896237863367179, 'precision': 0.9896912119277181, 'recall': 0.9896828429499427, 'loss': 0.023311286629563537, 'num_authors': 2617}\n",
      "Evaluating: 100%|██████████| 126/126 [02:33<00:00,  1.22s/it]\n",
      "INFO:src.models.trainer:Evaluated 586 unique authors, 413 with multiple batches\n",
      "INFO:src.models.trainer:Average batches per author: 6.86\n",
      "INFO:src.models.trainer:Validation metrics: {'accuracy': 0.9709897610921502, 'f1': 0.9705930576696971, 'precision': 0.9708904004151317, 'recall': 0.9709897610921502, 'auc': 0.9903726708074534, 'loss': 0.05419942415467212, 'num_authors': 586}\n",
      "INFO:src.models.trainer:\n",
      "Evaluating on test set...\n",
      "Evaluating: 100%|██████████| 112/112 [02:38<00:00,  1.41s/it]\n",
      "INFO:src.models.trainer:Evaluated 546 unique authors, 372 with multiple batches\n",
      "INFO:src.models.trainer:Average batches per author: 6.54\n",
      "INFO:src.models.trainer:Test metrics: {'accuracy': 0.9706959706959707, 'f1': 0.9700503185667579, 'precision': 0.9713408586668721, 'recall': 0.9706959706959707, 'auc': 0.9905437352245863, 'loss': 0.06178555041801052, 'num_authors': 546}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n",
      "\n",
      "Final metrics:\n",
      "accuracy: 0.9707\n",
      "f1: 0.9701\n",
      "precision: 0.9713\n",
      "recall: 0.9707\n",
      "auc: 0.9905\n",
      "loss: 0.0618\n",
      "num_authors: 546.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "final_metrics = trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(\"\\nFinal metrics:\")\n",
    "for metric, value in final_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Save final configuration and results\n",
    "results = {\n",
    "    'config': config,\n",
    "    'final_metrics': final_metrics\n",
    "}\n",
    "\n",
    "with open(CHECKPOINT_DIR / 'training_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b69dc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved final model to: checkpoints/best_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Save the final model state\n",
    "final_model_path = CHECKPOINT_DIR / 'best_model.pt'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"\\nSaved final model to: {final_model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "environment_variables": {},
   "parameters": {
    "product": {
     "metrics": "/home/luuka/thesis/workspace/checkpoints/best_model_info.json",
     "model": "/home/luuka/thesis/workspace/checkpoints/best_model.pt",
     "nb": "/home/luuka/thesis/workspace/output/02_train.ipynb",
     "results": "/home/luuka/thesis/workspace/checkpoints/training_results.json"
    }
   },
   "version": null
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
