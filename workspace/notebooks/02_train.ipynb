{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f48636d5",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# declare a list tasks whose products you want to use as inputs\n",
    "upstream = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e267ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Import custom modules\n",
    "from src.models.bert_model import TrollDetector\n",
    "from src.models.trainer import TrollDetectorTrainer\n",
    "from src.data_tools.dataset import TrollDataset, collate_batch\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c92cb16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "model_name: distilbert-base-multilingual-cased\n",
      "max_length: 64\n",
      "batch_size: 64\n",
      "learning_rate: 2e-05\n",
      "weight_decay: 0.01\n",
      "num_epochs: 3\n",
      "warmup_steps: 0\n",
      "max_grad_norm: 1.0\n",
      "comments_per_user: 5\n",
      "use_wandb: False\n",
      "random_state: 42\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "DATA_DIR = Path('../data')\n",
    "PROCESSED_DATA_DIR = DATA_DIR / 'processed'\n",
    "CHECKPOINT_DIR = Path('../checkpoints')\n",
    "\n",
    "# Create checkpoint directory\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training configuration\n",
    "config = {\n",
    "    'model_name': 'distilbert-base-multilingual-cased',\n",
    "    'max_length': 64,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 2e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'num_epochs': 3,\n",
    "    'warmup_steps': 0,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'comments_per_user': 5,\n",
    "    'use_wandb': False,\n",
    "    'random_state': 42  # Default if config not found\n",
    "}\n",
    "\n",
    "# Try to load preprocessing config\n",
    "try:\n",
    "    with open(PROCESSED_DATA_DIR / 'preprocessing_config.json', 'r') as f:\n",
    "        preproc_config = json.load(f)\n",
    "        config['random_state'] = preproc_config.get('random_state', 42)\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: preprocessing_config.json not found, using default random_state\")\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3767e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 1724555 samples, 16242 authors\n",
      "Val:   558573 samples, 3481 authors\n",
      "Test:  418812 samples, 3481 authors\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data splits\n",
    "train_df = pd.read_parquet(PROCESSED_DATA_DIR / 'train.parquet')\n",
    "val_df = pd.read_parquet(PROCESSED_DATA_DIR / 'val.parquet')\n",
    "test_df = pd.read_parquet(PROCESSED_DATA_DIR / 'test.parquet')\n",
    "\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"Train: {len(train_df)} samples, {train_df['author'].nunique()} authors\")\n",
    "print(f\"Val:   {len(val_df)} samples, {val_df['author'].nunique()} authors\")\n",
    "print(f\"Test:  {len(test_df)} samples, {test_df['author'].nunique()} authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45fe3bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets and DataLoaders\n",
    "# Initialize datasets\n",
    "train_dataset = TrollDataset(\n",
    "    train_df,\n",
    "    tokenizer_name=config['model_name'],\n",
    "    max_length=config['max_length'],\n",
    "    comments_per_user=config['comments_per_user']\n",
    ")\n",
    "\n",
    "val_dataset = TrollDataset(\n",
    "    val_df,\n",
    "    tokenizer_name=config['model_name'],\n",
    "    max_length=config['max_length'],\n",
    "    comments_per_user=config['comments_per_user']\n",
    ")\n",
    "\n",
    "test_dataset = TrollDataset(\n",
    "    test_df,\n",
    "    tokenizer_name=config['model_name'],\n",
    "    max_length=config['max_length'],\n",
    "    comments_per_user=config['comments_per_user']\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be1b89ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luuka/thesis/workspace/src/models/trainer.py:48: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model and Trainer\n",
    "model = TrollDetector(\n",
    "    model_name=config['model_name'],\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = TrollDetectorTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    learning_rate=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay'],\n",
    "    max_grad_norm=config['max_grad_norm'],\n",
    "    num_epochs=config['num_epochs'],\n",
    "    warmup_steps=config['warmup_steps'],\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    use_wandb=config['use_wandb']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e34eaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.models.trainer:Starting training on device: cuda\n",
      "INFO:src.models.trainer:Training samples: 16242\n",
      "INFO:src.models.trainer:Validation samples: 3481\n",
      "INFO:src.models.trainer:\n",
      "Epoch 1/3\n",
      "Training:   0%|                                                                     | 0/254 [00:00<?, ?it/s]/home/luuka/thesis/workspace/src/models/trainer.py:91: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Training: 100%|██████████████████████████████████████████████| 254/254 [01:13<00:00,  3.46it/s, loss=0.0089]\n",
      "INFO:src.models.trainer:Training metrics: {'accuracy': 0.9745105282600665, 'f1': 0.9729995098041248, 'precision': 0.9730301896986112, 'recall': 0.9745105282600665, 'loss': 0.08144478514555871}\n",
      "Evaluating: 100%|███████████████████████████████████████████████████████████| 55/55 [00:12<00:00,  4.39it/s]\n",
      "INFO:src.models.trainer:Validation metrics: {'accuracy': 0.986210858948578, 'f1': 0.9856956694655379, 'precision': 0.985924896791376, 'recall': 0.986210858948578, 'auc': 0.9928983690250293, 'loss': 0.0547953194684603}\n",
      "INFO:src.models.trainer:New best model with validation AUC: 0.9929\n",
      "INFO:src.models.trainer:\n",
      "Epoch 2/3\n",
      "Training:   0%|                                                                     | 0/254 [00:00<?, ?it/s]/home/luuka/thesis/workspace/src/models/trainer.py:91: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Training: 100%|██████████████████████████████████████████████| 254/254 [01:12<00:00,  3.50it/s, loss=0.0042]\n",
      "INFO:src.models.trainer:Training metrics: {'accuracy': 0.9905184090629233, 'f1': 0.9903956153355757, 'precision': 0.9903616718987694, 'recall': 0.9905184090629233, 'loss': 0.03143099409080748}\n",
      "Evaluating: 100%|███████████████████████████████████████████████████████████| 55/55 [00:13<00:00,  4.16it/s]\n",
      "INFO:src.models.trainer:Validation metrics: {'accuracy': 0.990807239299052, 'f1': 0.9908463972389845, 'precision': 0.9908958959899721, 'recall': 0.990807239299052, 'auc': 0.9973005331164724, 'loss': 0.03314073061020198}\n",
      "INFO:src.models.trainer:New best model with validation AUC: 0.9973\n",
      "INFO:src.models.trainer:\n",
      "Epoch 3/3\n",
      "Training:   0%|                                                                     | 0/254 [00:00<?, ?it/s]/home/luuka/thesis/workspace/src/models/trainer.py:91: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Training: 100%|██████████████████████████████████████████████| 254/254 [01:13<00:00,  3.47it/s, loss=0.0182]\n",
      "INFO:src.models.trainer:Training metrics: {'accuracy': 0.9926117473217584, 'f1': 0.9925311506116192, 'precision': 0.9925148390640434, 'recall': 0.9926117473217584, 'loss': 0.026237918474660143}\n",
      "Evaluating: 100%|███████████████████████████████████████████████████████████| 55/55 [00:13<00:00,  4.09it/s]\n",
      "INFO:src.models.trainer:Validation metrics: {'accuracy': 0.9902326917552428, 'f1': 0.9901469574855634, 'precision': 0.9901044921251865, 'recall': 0.9902326917552428, 'auc': 0.9963291768320232, 'loss': 0.03154707912313329}\n",
      "INFO:src.models.trainer:\n",
      "Evaluating on test set...\n",
      "Evaluating: 100%|███████████████████████████████████████████████████████████| 55/55 [00:12<00:00,  4.29it/s]\n",
      "INFO:src.models.trainer:Test metrics: {'accuracy': 0.993105429474289, 'f1': 0.9931522313471972, 'precision': 0.9932238042848721, 'recall': 0.993105429474289, 'auc': 0.9953298988194607, 'loss': 0.024184087150603193}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n",
      "\n",
      "Final metrics:\n",
      "accuracy: 0.9931\n",
      "f1: 0.9932\n",
      "precision: 0.9932\n",
      "recall: 0.9931\n",
      "auc: 0.9953\n",
      "loss: 0.0242\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "final_metrics = trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(\"\\nFinal metrics:\")\n",
    "for metric, value in final_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Save final configuration and results\n",
    "results = {\n",
    "    'config': config,\n",
    "    'final_metrics': final_metrics\n",
    "}\n",
    "\n",
    "with open(CHECKPOINT_DIR / 'training_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b69dc68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "environment_variables": {},
   "parameters": {
    "product": {
     "metrics": "/home/luuka/thesis/workspace/checkpoints/best_model_info.json",
     "model": "/home/luuka/thesis/workspace/checkpoints/best_model.pt",
     "nb": "/home/luuka/thesis/workspace/output/02_train.ipynb",
     "results": "/home/luuka/thesis/workspace/checkpoints/training_results.json"
    }
   },
   "version": null
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
