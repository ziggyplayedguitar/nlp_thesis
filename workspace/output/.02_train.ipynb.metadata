{"timestamp": 1743125429.316001, "stored_source_code": "# declare a list tasks whose products you want to use as inputs\nupstream = None\n\n# Imports and Setup\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport sys\nimport logging\nimport json\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Add project root to path\nsys.path.append(str(Path.cwd().parent))\n\n# Import custom modules\nfrom src.models.bert_model import TrollDetector\nfrom src.models.trainer import TrollDetectorTrainer\nfrom src.data_tools.dataset import TrollDataset, collate_batch\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n# Define paths\nDATA_DIR = Path('../data')\nPROCESSED_DATA_DIR = DATA_DIR / 'processed'\nCHECKPOINT_DIR = Path('../checkpoints')\n\n# Create checkpoint directory\nCHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n\n# Training configuration\nconfig = {\n    'model_name': 'distilbert-base-multilingual-cased',\n    'max_length': 128,\n    'batch_size': 64,\n    'learning_rate': 2e-5,\n    'weight_decay': 0.03,\n    'num_epochs': 3,\n    'dropout_rate': 0.2,\n    'warmup_steps': 50,\n    'max_grad_norm': 1.0,\n    'comments_per_user': 5,\n    'early_stopping_patience': 3,\n    'use_wandb': False,\n    'random_state': 17,  # Default if config not found\n    'label_smoothing': 0.1        # Added label smoothing\n}\n\n# Try to load preprocessing config\ntry:\n    with open(PROCESSED_DATA_DIR / 'preprocessing_config.json', 'r') as f:\n        preproc_config = json.load(f)\n        config['random_state'] = preproc_config.get('random_state', 42)\nexcept FileNotFoundError:\n    print(\"Warning: preprocessing_config.json not found, using default random_state\")\n\nprint(\"Configuration loaded:\")\nfor key, value in config.items():\n    print(f\"{key}: {value}\")\n# Load preprocessed data splits\ntrain_df = pd.read_parquet(PROCESSED_DATA_DIR / 'train.parquet')\nval_df = pd.read_parquet(PROCESSED_DATA_DIR / 'val.parquet')\ntest_df = pd.read_parquet(PROCESSED_DATA_DIR / 'test.parquet')\n\nprint(\"Dataset sizes:\")\nprint(f\"Train: {len(train_df)} samples, {train_df['author'].nunique()} authors\")\nprint(f\"Val:   {len(val_df)} samples, {val_df['author'].nunique()} authors\")\nprint(f\"Test:  {len(test_df)} samples, {test_df['author'].nunique()} authors\")\n# Create Datasets and DataLoaders\n# Initialize datasets\ntrain_dataset = TrollDataset(\n    train_df,\n    tokenizer_name=config['model_name'],\n    max_length=config['max_length'],\n    comments_per_user=config['comments_per_user']\n)\n\nval_dataset = TrollDataset(\n    val_df,\n    tokenizer_name=config['model_name'],\n    max_length=config['max_length'],\n    comments_per_user=config['comments_per_user']\n)\n\ntest_dataset = TrollDataset(\n    test_df,\n    tokenizer_name=config['model_name'],\n    max_length=config['max_length'],\n    comments_per_user=config['comments_per_user']\n)\n\n# Create dataloaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=config['batch_size'],\n    shuffle=True,\n    collate_fn=collate_batch\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=config['batch_size'],\n    shuffle=False,\n    collate_fn=collate_batch\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=config['batch_size'],\n    shuffle=False,\n    collate_fn=collate_batch\n)\n# Initialize Model and Trainer\nmodel = TrollDetector(\n    model_name=config['model_name'],\n    dropout_rate=config['dropout_rate']\n)\n\n# Initialize trainer\ntrainer = TrollDetectorTrainer(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    test_loader=test_loader,\n    learning_rate=config['learning_rate'],\n    weight_decay=config['weight_decay'],\n    max_grad_norm=config['max_grad_norm'],\n    num_epochs=config['num_epochs'],\n    warmup_steps=config['warmup_steps'],\n    checkpoint_dir=CHECKPOINT_DIR,\n    use_wandb=config['use_wandb']\n)\n# Train the model\nfinal_metrics = trainer.train()\n\nprint(\"\\nTraining completed!\")\nprint(\"\\nFinal metrics:\")\nfor metric, value in final_metrics.items():\n    print(f\"{metric}: {value:.4f}\")\n\n# Save final configuration and results\nresults = {\n    'config': config,\n    'final_metrics': final_metrics\n}\n\nwith open(CHECKPOINT_DIR / 'training_results.json', 'w') as f:\n    json.dump(results, f, indent=2)\n", "params": {}}