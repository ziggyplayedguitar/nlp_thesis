{"timestamp": 1743121698.972907, "stored_source_code": "# declare a list tasks whose products you want to use as inputs\nupstream = None\n\n# Imports and Setup\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport sys\nimport logging\nimport json\nfrom tqdm import tqdm\n\n# Add project root to path\nsys.path.append(str(Path.cwd().parent))\n\nfrom src.data_tools.preprocessor import load_and_clean_data\nfrom src.data_tools.dataset import create_data_splits\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n# Configuration\n# Define paths\nDATA_DIR = Path('../data')\nPROCESSED_DATA_DIR = DATA_DIR / 'processed'\n\n# Create directories if they don't exist\nPROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n\n# Model configuration\nconfig = {\n    'min_text_length': 5,\n    'train_size': 0.7,\n    'val_size': 0.15,\n    'test_size': 0.15,\n    'random_state': 42\n}\n\n# Save config\nwith open(PROCESSED_DATA_DIR / 'preprocessing_config.json', 'w') as f:\n    json.dump(config, f, indent=2)\n\nprint(\"Configuration saved!\")\n# Cell 3: Load and examine raw data\n# Load Czech comment data\ndf_raw = load_and_clean_data(str(DATA_DIR))\n\ndf_raw = df_raw.rename(columns={'account': 'author'})\ndf_raw = df_raw.rename(columns={'tweet': 'text'})\n\nprint(\"Raw dataset info:\")\nprint(f\"Total samples: {len(df_raw)}\")\nprint(f\"Columns: {df_raw.columns.tolist()}\")\nprint(\"\\nMissing values:\")\nprint(df_raw.isnull().sum())\n\n# Show class distribution\nprint(\"\\nClass distribution:\")\nprint(df_raw['troll'].value_counts(normalize=True))\nprint(\"Available columns:\", df_raw.columns.tolist())\n# Create Train/Val/Test Splits\n# Create splits ensuring no author overlap\ntrain_df, val_df, test_df = create_data_splits(\n    df_raw,\n    train_size=config['train_size'],\n    val_size=config['val_size'],\n    test_size=config['test_size'],\n    random_state=config['random_state']\n)\n\nprint(\"Dataset splits:\")\nprint(f\"Train: {len(train_df)} samples, {train_df['author'].nunique()} authors\")\nprint(f\"Val:   {len(val_df)} samples, {val_df['author'].nunique()} authors\")\nprint(f\"Test:  {len(test_df)} samples, {test_df['author'].nunique()} authors\")\n# Save Processed Data\n# Save splits to parquet\nfor split_name, split_df in [\n    ('train', train_df),\n    ('val', val_df),\n    ('test', test_df)\n]:\n    output_path = PROCESSED_DATA_DIR / f'{split_name}.parquet'\n    split_df.to_parquet(output_path, index=False)\n    print(f\"Saved {split_name} split to {output_path}\")\n\n# Save preprocessing config\nimport json\nconfig_path = PROCESSED_DATA_DIR / 'preprocessing_config.json'\nwith open(config_path, 'w') as f:\n    json.dump(config, f, indent=2)\nprint(f\"\\nSaved preprocessing config to {config_path}\")\n", "params": {}}