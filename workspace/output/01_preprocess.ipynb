{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b62204fc",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [5]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "780d88a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:20:19.993266Z",
     "iopub.status.busy": "2025-04-11T16:20:19.992814Z",
     "iopub.status.idle": "2025-04-11T16:20:19.996198Z",
     "shell.execute_reply": "2025-04-11T16:20:19.995882Z"
    },
    "papermill": {
     "duration": 0.006117,
     "end_time": "2025-04-11T16:20:19.996827",
     "exception": false,
     "start_time": "2025-04-11T16:20:19.990710",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# declare a list tasks whose products you want to use as inputs\n",
    "upstream = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd798381",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:20:20.003606Z",
     "iopub.status.busy": "2025-04-11T16:20:20.003369Z",
     "iopub.status.idle": "2025-04-11T16:20:20.005483Z",
     "shell.execute_reply": "2025-04-11T16:20:20.005151Z"
    },
    "papermill": {
     "duration": 0.003947,
     "end_time": "2025-04-11T16:20:20.006076",
     "exception": false,
     "start_time": "2025-04-11T16:20:20.002129",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "product = {\"nb\": \"/home/luuka/thesis/workspace/output/01_preprocess.ipynb\", \"train\": \"/home/luuka/thesis/workspace/data/processed/train.parquet\", \"val\": \"/home/luuka/thesis/workspace/data/processed/val.parquet\", \"test\": \"/home/luuka/thesis/workspace/data/processed/test.parquet\", \"config\": \"/home/luuka/thesis/workspace/data/processed/preprocessing_config.json\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab5e452e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:20:20.008044Z",
     "iopub.status.busy": "2025-04-11T16:20:20.007856Z",
     "iopub.status.idle": "2025-04-11T16:20:22.444109Z",
     "shell.execute_reply": "2025-04-11T16:20:22.443724Z"
    },
    "papermill": {
     "duration": 2.438217,
     "end_time": "2025-04-11T16:20:22.445032",
     "exception": false,
     "start_time": "2025-04-11T16:20:20.006815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from src.data_tools.preprocessor import load_and_clean_data\n",
    "from src.data_tools.dataset import create_data_splits\n",
    "from src.data_tools.preprocessor import save_data_to_json\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b89778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:20:22.456547Z",
     "iopub.status.busy": "2025-04-11T16:20:22.456226Z",
     "iopub.status.idle": "2025-04-11T16:20:22.459429Z",
     "shell.execute_reply": "2025-04-11T16:20:22.459063Z"
    },
    "papermill": {
     "duration": 0.014009,
     "end_time": "2025-04-11T16:20:22.460068",
     "exception": false,
     "start_time": "2025-04-11T16:20:22.446059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved!\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "# Define paths\n",
    "DATA_DIR = Path('data')\n",
    "PROCESSED_DATA_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model configuration\n",
    "config = {\n",
    "    'min_text_length': 5,\n",
    "    'train_size': 0.7,\n",
    "    'val_size': 0.15,\n",
    "    'test_size': 0.15,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Save config\n",
    "with open(PROCESSED_DATA_DIR / 'preprocessing_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Configuration saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15926d4",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d38b39b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:20:22.462282Z",
     "iopub.status.busy": "2025-04-11T16:20:22.462078Z",
     "iopub.status.idle": "2025-04-11T16:20:31.629197Z",
     "shell.execute_reply": "2025-04-11T16:20:31.628659Z"
    },
    "papermill": {
     "duration": 9.168939,
     "end_time": "2025-04-11T16:20:31.629840",
     "exception": true,
     "start_time": "2025-04-11T16:20:22.460901",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_tools.preprocessor:Loading Russian troll tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_tools.preprocessor:Loading Sentiment140 tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_tools.preprocessor:Loading celebrity tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_tools.preprocessor:Loading manualy scraped tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_tools.preprocessor:Loading Twitter JSON data from non_troll_politics folder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data_tools.preprocessor:Loading information operations tweets...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['language'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load Czech comment data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_clean_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m df_raw\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccount\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m      5\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m df_raw\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "File \u001b[0;32m~/thesis/workspace/src/data_tools/preprocessor.py:125\u001b[0m, in \u001b[0;36mload_and_clean_data\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m    123\u001b[0m parquet_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(parquet_folder\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    124\u001b[0m parquet_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mread_parquet(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m parquet_files])\n\u001b[0;32m--> 125\u001b[0m parquet_data \u001b[38;5;241m=\u001b[39m \u001b[43mparquet_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccountid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mis_control\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlanguage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    126\u001b[0m parquet_data\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccountid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccount\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost_text\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    127\u001b[0m parquet_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtroll\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mparquet_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_control\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# troll is True when is_control is False\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['language'] not in index\""
     ]
    }
   ],
   "source": [
    "# Load Czech comment data\n",
    "df_raw = load_and_clean_data(str(DATA_DIR))\n",
    "\n",
    "df_raw = df_raw.rename(columns={'account': 'author'})\n",
    "df_raw = df_raw.rename(columns={'tweet': 'text'})\n",
    "\n",
    "print(\"Raw dataset info:\")\n",
    "print(f\"Total samples: {len(df_raw)}\")\n",
    "print(f\"Columns: {df_raw.columns.tolist()}\")\n",
    "print(\"\\nMissing values:\")\n",
    "print(df_raw.isnull().sum())\n",
    "\n",
    "# Show class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df_raw['troll'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f873a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Available columns:\", df_raw.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928b8683",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Train/Val/Test Splits\n",
    "# Create splits ensuring no author overlap\n",
    "train_df, val_df, test_df = create_data_splits(\n",
    "    df_raw,\n",
    "    train_size=config['train_size'],\n",
    "    val_size=config['val_size'],\n",
    "    test_size=config['test_size'],\n",
    "    random_state=config['random_state']\n",
    ")\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "print(f\"Train: {len(train_df)} samples, {train_df['author'].nunique()} authors\")\n",
    "print(f\"Val:   {len(val_df)} samples, {val_df['author'].nunique()} authors\")\n",
    "print(f\"Test:  {len(test_df)} samples, {test_df['author'].nunique()} authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f05ff77",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save Processed Data\n",
    "# Save splits to parquet\n",
    "for split_name, split_df in [\n",
    "    ('train', train_df),\n",
    "    ('val', val_df),\n",
    "    ('test', test_df)\n",
    "]:\n",
    "    output_path = PROCESSED_DATA_DIR / f'{split_name}.parquet'\n",
    "    split_df.to_parquet(output_path, index=False)\n",
    "    print(f\"Saved {split_name} split to {output_path}\")\n",
    "\n",
    "# Save preprocessing config\n",
    "import json\n",
    "config_path = PROCESSED_DATA_DIR / 'preprocessing_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f\"\\nSaved preprocessing config to {config_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "duration": 13.571011,
   "end_time": "2025-04-11T16:20:32.877363",
   "exception": true,
   "input_path": "/tmp/tmpb1ux1z2n.ipynb",
   "output_path": "/home/luuka/thesis/workspace/output/01_preprocess.ipynb",
   "parameters": {
    "product": {
     "config": "/home/luuka/thesis/workspace/data/processed/preprocessing_config.json",
     "nb": "/home/luuka/thesis/workspace/output/01_preprocess.ipynb",
     "test": "/home/luuka/thesis/workspace/data/processed/test.parquet",
     "train": "/home/luuka/thesis/workspace/data/processed/train.parquet",
     "val": "/home/luuka/thesis/workspace/data/processed/val.parquet"
    }
   },
   "start_time": "2025-04-11T16:20:19.306352"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}