{"timestamp": 1743125703.632922, "stored_source_code": "# declare a list tasks whose products you want to use as inputs\nupstream = None\n\n# Imports and Setup\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport sys\nimport logging\nimport json\nimport torch\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport importlib\n\n# Add project root to path\nsys.path.append(str(Path.cwd().parent))\n\n# Import and reload modules to get fresh changes\nfrom src.models.predictor import TrollPredictor\nfrom src.data_tools.czech_data_tools import load_czech_media_data\nfrom src.data_tools.czech_data_tools import export_comments_by_prediction\nimport src.models.predictor\n\n# Configure matplotlib and seaborn\nplt.style.use('default')\nsns.set_theme()\nDATA_DIR = Path('../data')\n\n# Initialize predictor with model\nmodel_path = \"../checkpoints/best_model.pt\" if 'model_path' not in locals() else \"../\" + model_path\n\npredictor = TrollPredictor(\n    model_path=model_path,\n    comments_per_user=5,\n    max_length=64\n)\n# Check if preprocessed data exists and load it, otherwise process from source\nPROCESSED_DIR = DATA_DIR / 'processed'\nPROCESSED_DIR.mkdir(exist_ok=True)\nczech_media_parquet = PROCESSED_DIR / 'czech_media_comments.parquet'\n\nif czech_media_parquet.exists():\n    print(f\"Loading preprocessed data from {czech_media_parquet}...\")\n    czech_comments = pd.read_parquet(czech_media_parquet)\nelse:\n    print(\"Preprocessed data not found. Loading and processing from source...\")\n    czech_comments = load_czech_media_data(str(DATA_DIR / 'MediaSource'))\n    # Save processed data (we'll keep cell 6 in case you want to save explicitly)\n    czech_comments.to_parquet(czech_media_parquet)\n    print(f\"Preprocessed data saved to {czech_media_parquet}\")\n\nprint(f\"Loaded {len(czech_comments)} comments from {czech_comments['author'].nunique()} unique authors\")\n# Display data info\nprint(\"\\nDataset columns:\", czech_comments.columns.tolist())\nprint(\"\\nSample of comments:\")\nprint(czech_comments[['text', 'author', 'sentiment']].head())\n# Count original number of authors and comments\ntotal_authors = czech_comments['author'].nunique()\ntotal_comments = len(czech_comments)\nprint(f\"Loaded {total_comments} comments from {total_authors} unique authors\")\n\n# Filter authors with at least 5 comments\nauthor_counts = czech_comments['author'].value_counts()\nauthors_with_min_5 = author_counts[author_counts >= 5].index\nczech_comments = czech_comments[czech_comments['author'].isin(authors_with_min_5)]\n\n# Count filtered number of authors and comments\nfiltered_authors = czech_comments['author'].nunique()\nfiltered_comments = len(czech_comments)\nprint(f\"Filtered to {filtered_comments} comments from {filtered_authors} authors with at least 5 comments\")\nprint(f\"Removed {total_authors - filtered_authors} authors with fewer than 5 comments\")\n# Make predictions and save results\noutput_dir = Path('../output')\noutput_dir.mkdir(exist_ok=True)\ncache_file = output_dir / 'results_df.pkl'\n\nresults = []\nfor idx, group in tqdm(czech_comments.groupby('author')):\n    texts = group['text'].tolist()\n    \n    # Get prediction\n    pred = predictor.predict_batch(texts)\n    \n    # Store results\n    results.append({\n        'author': idx,\n        'n_comments': len(texts),\n        'prediction': pred['prediction'],\n        'confidence': pred['confidence'],\n        'troll_probability': pred['probabilities'][1]\n    })\n\n# Convert to DataFrame and cache it\nresults_df = pd.DataFrame(results)\nresults_df.to_pickle(cache_file)\nprint(\"results_df saved for future runs.\")\n# Cell: Analyze predictions by confidence levels\n# Create visualization\nplt.figure(figsize=(10, 6))\nsns.histplot(\n    data=results_df, \n    x='confidence', \n    hue='prediction', \n    bins=20\n)\nplt.title('Prediction Confidence Distribution')\nplt.xlabel('Confidence Score')\nplt.ylabel('Count')\nplt.tight_layout()\nplt.show()\n\n\n\n# Count plot of authors per prediction group\nplt.subplot(1, 2, 1)\nsns.countplot(data=results_df, x='prediction', hue='prediction', palette=['#FF9999','#99CCFF'],legend=False)\nplt.title('Number of Authors by Troll Classification')\nplt.xlabel('Prediction')\nplt.ylabel('Count')\n\n# Boxplot for confidence distribution per prediction group\nplt.subplot(1, 2, 2)\nsns.boxplot(data=results_df, x='prediction', hue='prediction', y='confidence', palette=['#FF9999','#99CCFF'], legend=False)\nplt.title('Confidence Distribution by Author Prediction')\nplt.xlabel('Prediction')\nplt.ylabel('Confidence Score')\n\n# Distribution of troll probability scores\nplt.figure(figsize=(8, 6))\nsns.histplot(data=results_df, x='troll_probability', bins=20, kde=True, color='#99CCFF', legend=False)\nplt.title('Distribution of Troll Probability Scores')\nplt.xlabel('Troll Probability')\nplt.ylabel('Count')\nplt.tight_layout()\nplt.show()\n# Save a sample of troll and not_troll classified comments\n\nexport_comments_by_prediction(\n    predictions_df=results_df,\n    comments_df=czech_comments,\n    prediction_class='troll',\n    min_confidence=0.7,\n    max_confidence=0.9,\n    max_authors=50,\n    output_file=\"../output/troll_comments.json\"\n)\n\nexport_comments_by_prediction(\n    predictions_df=results_df,\n    comments_df=czech_comments,\n    prediction_class='not_troll',\n    min_confidence=0.8,\n    max_authors=50,\n    output_file=\"../output/non_troll_comments.json\"\n)\ndef explain_author(\n    author_name: str, \n    predictor_instance: TrollPredictor, \n    comments_df: pd.DataFrame, \n    display_plots: bool = True,\n):\n    \"\"\"Interactive function to explain why an author was classified as a troll using multiple methods\"\"\"\n    \n    # Get author's comments\n    author_comments = comments_df[comments_df['author'] == author_name]['text'].tolist()\n    \n    if not author_comments:\n        print(f'No comments found for author: {author_name}')\n        return\n        \n    # Get prediction first\n    pred = predictor_instance.predict_batch(author_comments)\n    \n    if pred['prediction'] != 'troll':\n        print(f'Author {author_name} is not classified as a troll (confidence: {pred[\"confidence\"]:.3f})')\n        return\n    \n    print(f\"\\nAnalyzing author: {author_name}\")\n    print(f\"Prediction confidence: {pred['confidence']:.3f}\")\n    print(f\"Troll probability: {pred['probabilities'][1]:.3f}\")\n    \n    # Generate occlusion sensitivity explanation\n    print(\"\\n=== Occlusion Sensitivity Analysis ===\")\n    explanation = predictor_instance.explain_prediction(author_comments)\n    \n    # Print occlusion sensitivity results\n    for i, exp in enumerate(explanation['explanations'], 1):\n        print(f\"\\nComment {i}:\")\n        print(f\"Text: {exp['tweet_text']}\")\n        print(\"\\nMost influential tokens (Occlusion Sensitivity):\")\n        \n        # Create a table with token contributions\n        contrib_data = []\n        for contrib in exp['token_contributions']:\n            direction = \"Supporting\" if contrib['contribution'] == 'positive' else \"Opposing\"\n            contrib_data.append([\n                contrib['token'],\n                f\"{contrib['importance']:.4f}\",\n                direction\n            ])\n        \n        contrib_df = pd.DataFrame(contrib_data, columns=['Token', 'Importance', 'Effect'])\n        print(contrib_df.to_string(index=False))\n        \n        if display_plots:\n            img = plt.imread(exp['plot_filename'])\n            plt.figure(figsize=(12, 4))\n            plt.imshow(img)\n            plt.axis('off')\n            plt.title(f'Occlusion Sensitivity Analysis - Comment {i}')\n            plt.show()\n        \n        print(\"-\" * 80)\n# Example usage\n# authors_to_explain = [\n#     \"Jitka B\u00e1rtov\u00e1\",\n#     \"Roman My\u0161ka\",   \n# ]\nauthors_to_explain = [\n    \"Josef H\u016fzl\",\n    \"Johny Death\",   \n]\n\n\nfor author in authors_to_explain:\n    explain_author(\n        author_name=author,\n        predictor_instance=predictor,\n        comments_df=czech_comments,\n        display_plots=True,\n    )\n    print(\"\\n\" + \"=\"*80 + \"\\n\")\n# List names of medium confidence trolls\nmedium_conf_trolls = results_df[\n    (results_df['confidence'] >= 0.6) & \n    (results_df['confidence'] < 0.7) & \n    (results_df['prediction'] == 'troll')\n]\n\n# Display the names\nmedium_conf_troll_names = medium_conf_trolls['author'].tolist()\nprint(\"Medium confidence trolls (confidence between 0.6 and 0.7):\")\nprint(medium_conf_troll_names)\n", "params": {"model_path": "checkpoints/best_model.pt"}}