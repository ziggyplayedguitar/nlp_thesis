{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4140,"sourceType":"datasetVersion","datasetId":2477},{"sourceId":62094,"sourceType":"datasetVersion","datasetId":39964},{"sourceId":800048,"sourceType":"datasetVersion","datasetId":241}],"dockerImageVersionId":30176,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"My work with Russian Troll Tweets is divided in 3 parts due to Kaggle resources restrictions. Here are the part links:\n#### [Part 1. EDA](https://www.kaggle.com/code/mathemilda/part-i-eda)\n#### [Part 2. Feature Engineering](https://www.kaggle.com/code/mathemilda/part-2-feature-engineering)\n#### [Part 3. Machine Learning with accuracy 99.6%](https://www.kaggle.com/code/mathemilda/part-3-machine-learning-with-accuracy-99-6) (this one)\n\n# Outline for Part 3.\n## 1. Download, intial cleaning and concatination of data sets\n## 2. Feature Engineering for each account\n## 3. Feature Selection\n## 4. Machine Learning\n\n### My findings for this part\n* My plan was to use Deep Learning for this data set. I looked up the work of others and checked the data myself. I discovered that the approach did not yield good classification results, so I decided to add more features to create weak predictors. My \"weak\" predictor showed up as rather strong. \n* It turned out that the most prominent properties are the ones related to propaganda methods. Apparently trolls have specified guidelines and they stick to them.  I see it as convenient because we can set up filters for catching the most significant phenomena, and then check a whole account activity. \n* In addition the most important for prediction features turned out to be not very dependable on languages but mostly on troll account activity. Thus we can do it for other languages, and do not limit it to Russian trolls posting English texts.\n\n___\n*Remark.* For some reasons a very useful module `ftfy` cannot be found on Kaggle, although it was here a couple of years ago. I have tried to reach the Support team about it and they replied that I should look up the Kaggle forum for help. I installed it here, although as you can see I got a message that I should not do it.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport glob\nimport numpy as np\nimport re\nfrom string import punctuation, whitespace\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n!pip install ftfy\nimport ftfy\nfrom sklearn.utils import shuffle\nimport gc\nimport multiprocess as mp\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, \\\n        precision_score, recall_score, roc_auc_score\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.base import BaseEstimator, TransformerMixin","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-17T18:03:51.639694Z","iopub.execute_input":"2022-05-17T18:03:51.640268Z","iopub.status.idle":"2022-05-17T18:04:06.718588Z","shell.execute_reply.started":"2022-05-17T18:03:51.640177Z","shell.execute_reply":"2022-05-17T18:04:06.717222Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data download from Russian Troll Tweets, Sentiment140 and Celebrity tweets.\nI discovered the following problems with Russian Troll Tweets dataset, see https://www.kaggle.com/code/mathemilda/part-i-eda/notebook: \n* I have one missing tweet.\n* There are columns which I do not need.\n* I have many non English languages which I removed.\n* There are German and Russian texts among tweets classified as English.","metadata":{}},{"cell_type":"code","source":"#../input/russian-troll-tweets/IRAhandle_tweets_1.csv\nPATH = \"../input/russian-troll-tweets/\"\nfilenames = glob.glob(os.path.join(PATH, \"*.csv\"))\nfull_ru_trolls = pd.concat((pd.read_csv(f) for f in filenames))\nfull_ru_trolls.drop(['external_author_id', 'region', 'harvested_date',\n        'updates', 'account_type', 'new_june_2018', 'post_type',\n        'account_category', 'following', 'followers', 'retweet'],\n        axis=1, inplace=True)\nfull_ru_trolls = full_ru_trolls[full_ru_trolls.content.notnull()]\nfull_ru_trolls['troll']=1\nfull_ru_trolls_en = full_ru_trolls[full_ru_trolls.language == 'English'].copy(deep=True)\nfull_ru_trolls_en.rename(columns={'author': 'account', 'content': 'tweet'}, inplace=True)\nfull_ru_trolls_en = full_ru_trolls_en[~full_ru_trolls_en.tweet.str.contains('А-Яа-я')]\ngerman_s = re.compile('(Ich )|(Sie )|(Ihnen )|( sich$)|( [Kk]?eine? )|( [Dd]as )|'+\n           '^[Dd]as |^[Ss]ind | bist | und | sind | (?!(van|von|-)) der |' + \n           '[ ^][a-z]*ö|[ ^][a-z]*ä|[ ^][a-z]*ü')\nfull_ru_trolls_en = full_ru_trolls_en[~full_ru_trolls_en.tweet.str.contains(german_s)].copy(deep=True)\ndel full_ru_trolls\nfull_ru_trolls_en.drop(['language', 'publish_date'],\n        axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T18:04:06.720622Z","iopub.execute_input":"2022-05-17T18:04:06.720881Z","iopub.status.idle":"2022-05-17T18:06:50.082936Z","shell.execute_reply.started":"2022-05-17T18:04:06.720853Z","shell.execute_reply":"2022-05-17T18:06:50.080973Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In datasets provided information is not consistent and column names differ for the same entities from one dataset to another. I changed all account name columns to 'account' and all tweet columns to 'tweet'. I'm not going to use other columns.\n\nHere I download Sentiment140 and Celebrity tweets I found on Kaggle. ","metadata":{}},{"cell_type":"code","source":"sentiment140 = pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv',\\\n    encoding = 'Latin-1', names=('target', 'id', 'date', 'flag', 'username','tweet'))\nsentiment140.drop(['target', 'id', 'date', 'flag'], axis=1, inplace=True)\nsentiment140.rename(columns={'username': 'account'}, inplace=True)\nsentiment140['troll']=0\n\nPATH = \"../input/RawTwitterFeeds\"\nfilenames = glob.glob(os.path.join(PATH, \"*.csv\"))\ncelebs = pd.concat((pd.read_csv(f) for f in filenames))\ncelebs.drop(['Unnamed: 0', 'Unnamed: 0.1','id', 'date', 'link', 'retweet'], axis=1,inplace=True)\ncelebs.rename(columns={'author': 'account', 'text': 'tweet'}, inplace=True)\ncelebs = celebs[celebs.tweet.notnull()]\ncelebs['troll']=0","metadata":{"execution":{"iopub.status.busy":"2022-05-17T18:06:50.086659Z","iopub.execute_input":"2022-05-17T18:06:50.087037Z","iopub.status.idle":"2022-05-17T18:07:00.665831Z","shell.execute_reply.started":"2022-05-17T18:06:50.087002Z","shell.execute_reply":"2022-05-17T18:07:00.664165Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now let us gather all in one data frame.","metadata":{}},{"cell_type":"code","source":"used_cols = ['account', 'tweet', 'troll']\ntotal_data = pd.concat([celebs[used_cols], sentiment140[used_cols], full_ru_trolls_en[used_cols]], \\\n                     ignore_index = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T18:07:00.668548Z","iopub.execute_input":"2022-05-17T18:07:00.668875Z","iopub.status.idle":"2022-05-17T18:07:01.058613Z","shell.execute_reply.started":"2022-05-17T18:07:00.668841Z","shell.execute_reply":"2022-05-17T18:07:01.056967Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The function below preprocesses a tweet, extracts properties and removes some things to get a more or less clean text. Look Part 2 for more detail on why I chose the particular properties.","metadata":{}},{"cell_type":"code","source":"dashes = [chr(int(d, 16)) for d in ['058A', '05BE', '1400', '1806', '2010', '2011',\\\n          '2012', '2013', '2014', '2015', '2053', '207B', '208B', '2212', '2E17', \\\n          '2E1A', '2E3A', '2E3B', '2E40', '2E5D', '301C', '3030', '30A0', 'FE31', \\\n          'FE32', 'FE58', 'FE63', 'FF0D', '10EAD']]\ndashes_compiled = re.compile('[' + ''.join(dashes) + ']+', flags = re.UNICODE)\n\ndef cleaning_and_counts(s):\n    s = ftfy.fix_text(s)\n    s = re.sub(dashes_compiled, '-', s) # all dashes should be the same\n    url_n = len(re.findall('https?://\\\\S+\\\\b', s)) # count urls\n    s = re.sub('https?://\\\\S+\\\\b', '', s) # and remove them\n    hasht_n = len(re.findall(r'#\\w+\\b', s)) # count hashtags\n    s = re.sub(r'#\\w+\\b', '', s) # remove them\n    handle_n = len(re.findall(r'@\\w{1,15}\\b', s)) # count handles\n    s = re.sub(r'@\\w{1,15}\\b', '', s) # remove them\n    s = re.sub('pic\\\\.twitter\\\\.com/\\\\w+\\\\b', '', s) # remove pictures. Counting them was useless\n    s = re.sub('\\\\s+', ' ', s) #reducing multiple whitespaces to one\n    s = s.lstrip(whitespace+punctuation+'\\xa0'+chr(8230)) # removing possible whitespaces in front\n    s = s.rstrip(whitespace+'\\xa0') # and on the back\n    l=''\n    emoj_and_such = 0\n    for ch in s:\n        if ord(ch) < 8204:\n            l += ch # keep a symbol if not emoji or pictogram or such\n        else:\n            emoj_and_such += 1 # counting emojis and pictograms\n    comma_n = len(re.findall(',', s))\n    exl_n =  len(re.findall('!', s))\n    dash_n = len(re.findall('-', s))\n    a_an_n = len(re.findall(r'\\b[Aa]n?\\b', s))\n    the_n = len(re.findall(r'\\b[Tt]he\\b', s))\n    # reduce a number of repeated symbols to no more than 2 \n    l = re.sub(r'(.)\\1\\1+', r'\\1\\1', l)\n    length = len(l)\n    words = [len(w) for w in re.findall(r'\\b\\w+\\b', l)]\n    if len(words)==0:\n        average_word = 0\n    else:\n        average_word = np.max(words)\n    return l, url_n, hasht_n, handle_n, emoj_and_such, exl_n, comma_n, dash_n\\\n        , a_an_n, the_n, length, average_word\n","metadata":{"execution":{"iopub.status.busy":"2022-05-17T18:07:01.06089Z","iopub.execute_input":"2022-05-17T18:07:01.061193Z","iopub.status.idle":"2022-05-17T18:07:01.082557Z","shell.execute_reply.started":"2022-05-17T18:07:01.061162Z","shell.execute_reply":"2022-05-17T18:07:01.081068Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nwith mp.Pool(processes= mp.cpu_count()) as p:\n    total_data['tuple'] = p.map(cleaning_and_counts, total_data.tweet)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T18:07:01.084115Z","iopub.execute_input":"2022-05-17T18:07:01.084337Z","iopub.status.idle":"2022-05-17T18:13:26.546421Z","shell.execute_reply.started":"2022-05-17T18:07:01.084309Z","shell.execute_reply":"2022-05-17T18:13:26.544275Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I need to distribute the created features in separate columns. I chose the data type for intergers as compact as I could.","metadata":{}},{"cell_type":"code","source":"features = (\"cleaned_tweet, url_n, hasht_n, handle_n, emoji_and_such, exl_n, \" +\n    \"comma_n, dash_n, a_an_n, the_n, length, average_word\").split(', ')\n\nfor i in range(len(features)):\n    if i ==0:\n        total_data[features[i]] = total_data.tuple.apply(lambda t: t[i])\n    else:\n        total_data[features[i]] = total_data.tuple.apply(lambda t: t[i]).astype(np.uint8)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T18:13:26.549188Z","iopub.execute_input":"2022-05-17T18:13:26.54954Z","iopub.status.idle":"2022-05-17T18:13:58.726512Z","shell.execute_reply.started":"2022-05-17T18:13:26.549496Z","shell.execute_reply":"2022-05-17T18:13:58.725246Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let us check the results and do some memory cleaning.","metadata":{}},{"cell_type":"code","source":"print(total_data.columns)\ntotal_data.drop(['tuple'], axis=1,inplace=True)\ndel full_ru_trolls_en, celebs, sentiment140\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T18:13:58.728208Z","iopub.execute_input":"2022-05-17T18:13:58.728465Z","iopub.status.idle":"2022-05-17T18:14:00.92946Z","shell.execute_reply.started":"2022-05-17T18:13:58.728432Z","shell.execute_reply":"2022-05-17T18:14:00.928099Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I would like to drop accounts which have less than 10 posts because I analize account activities, and 1 post does not give much information about it.","metadata":{}},{"cell_type":"code","source":"min_count = 10\nacc_properties = total_data[['account', 'troll']].groupby(['account'])\\\n    .agg(tweet_count=('account', 'size'),troll = ('troll','min'))\\\n    .reset_index()\nkept_accs = acc_properties[acc_properties.tweet_count >= min_count]\nrestricted = total_data[total_data.account.isin(kept_accs.account)].copy(deep=True)\ndel total_data\nnum_cols = features[1:]\nrestricted.drop(['tweet', 'cleaned_tweet'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T18:14:00.931945Z","iopub.execute_input":"2022-05-17T18:14:00.932279Z","iopub.status.idle":"2022-05-17T18:14:07.733279Z","shell.execute_reply.started":"2022-05-17T18:14:00.932247Z","shell.execute_reply":"2022-05-17T18:14:07.732113Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The function below computes percentiles for each count variable. I wish I could add it to the sklearn Pipeline, but I discovered that I cannot aggregate the output column because for estimator only the provided `y` value is used, and number of rows in `X` variables should be the same as `y` length, otherwise the Pipeline does not work.\n\nAfterwards percentiles are computed and the whole dataset is shuffled.","metadata":{}},{"cell_type":"code","source":"%%time\ndef percentile_calc(data, groupby_col, num_cols, percentile_list):\n    non_numeric = [col_name for col_name in data.columns if col_name not in num_cols]\n    for qu in percentile_list: \n        percentiles = data.groupby(groupby_col).quantile(q=qu/100).reset_index()\n        cols_to_change = {col : col +'_' + str(qu) for col in num_cols}\n        percentiles.rename(columns=cols_to_change, inplace=True)\n        if qu == percentile_list[0]:\n            all_percentiles = percentiles\n        else:\n            all_percentiles = pd.merge(all_percentiles, percentiles, how = \"left\",\\\n                                       on = non_numeric)\n    return all_percentiles\n\nall_percentiles = percentile_calc(restricted[['account', 'troll']+num_cols], \\\n                                 groupby_col='account', num_cols=num_cols,\n                                 percentile_list=range(10, 100, 10))\n    \nnew_features = all_percentiles.columns[2:]\nall_percentiles = shuffle(all_percentiles).reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T18:14:07.736096Z","iopub.execute_input":"2022-05-17T18:14:07.736321Z","iopub.status.idle":"2022-05-17T18:14:38.525844Z","shell.execute_reply.started":"2022-05-17T18:14:07.736289Z","shell.execute_reply":"2022-05-17T18:14:38.524916Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This is my customized scikit-learn Transformer for feature reduction using Mutual Information method and Pearson correlation, see Part 2 for explanation. The `X` variable must be a Pandas dataframe.  The Transformer drops one of the correlated columns while keeping the variable with higher Mutual Information in each correlated pair. There are options for minimal MI score, maximum correlation value and n_neighbors parameter for MI method.","metadata":{}},{"cell_type":"code","source":"def _drop_correlated(data, score_ordered_cols, max_corr, method='pearson'):\n    new = [[score_ordered_cols[0]], [0]]\n    corr_matrix = data[score_ordered_cols].corr(method).values\n    N = len(score_ordered_cols)\n    for i in range(1, N):\n        tr = corr_matrix[new[1], i]\n        if sum(np.abs(tr) > max_corr) == 0:\n            new[0] += [score_ordered_cols[i]]\n            new[1] += [i]\n    return new[0]\n\n\nclass feature_reduction(BaseEstimator, TransformerMixin):\n    def __init__(self, min_mi=.001, max_corr=.7, \\\n                 n_neighbors=11):\n        self.min_mi = min_mi\n        self.max_corr = max_corr\n        self.n_neighbors = n_neighbors\n\n    def fit(self, X, y):\n        X = X.copy(deep=True)\n        columns = X.columns\n        mi = mutual_info_classif(X.values, y, n_neighbors= self.n_neighbors)\n        cols_mi = list(zip(columns, mi))\n        cols_mi.sort(reverse=True, key=lambda x: x[1])\n        cols_mi = [pair[0] for pair in cols_mi if \\\n                   pair[1] > self.min_mi]\n        new_cols = _drop_correlated(X[cols_mi], cols_mi, \\\n                                   max_corr=self.max_corr)\n        self.selected_cols = new_cols\n        return self\n\n    def transform(self, X, y=None):\n        return X[self.selected_cols]","metadata":{"execution":{"iopub.status.busy":"2022-05-17T18:14:38.527392Z","iopub.execute_input":"2022-05-17T18:14:38.52767Z","iopub.status.idle":"2022-05-17T18:14:38.541902Z","shell.execute_reply.started":"2022-05-17T18:14:38.527631Z","shell.execute_reply":"2022-05-17T18:14:38.541086Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here go functions for printing out my Grid Search results.","metadata":{}},{"cell_type":"code","source":"def searchcv_best(estimator):\n    print(\" The best AUC score on a train set is:\\n\",\n          estimator.best_score_)\n    print(\" The best parameters are:\\n\",\n          estimator.best_params_)\ndef scores(y_test, y_pred):\n    print('Here go metrics on a test set.')\n    print(\"A confusion matrix is:\")\n    cm = confusion_matrix(y_test, y_pred)\n    print(cm)\n    print('accuracy:', accuracy_score(y_test, y_pred), \n          '\\nf1:', f1_score(y_test, y_pred),\n          '\\nprecision:', precision_score(y_test, y_pred), \n          '\\nrecall:', recall_score(y_test, y_pred), \n          '\\nroc_auc:', roc_auc_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-05-17T18:14:38.543285Z","iopub.execute_input":"2022-05-17T18:14:38.543691Z","iopub.status.idle":"2022-05-17T18:14:38.566976Z","shell.execute_reply.started":"2022-05-17T18:14:38.543653Z","shell.execute_reply":"2022-05-17T18:14:38.56589Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And there is a function for the Pipeline with Grid Search. I decided to apply the CatBoost method, created in Russia, because I feel that this is ironic.","metadata":{}},{"cell_type":"code","source":"def train_pipe_cv(data, columns, parameters, fr_params, cbc_params,\\\n                  cv=5, train_size=.8, scoring='roc_auc'):\n    X_train, X_test, y_train, y_test = train_test_split(data[columns], \\\n            data['troll'], train_size = train_size, stratify = data['troll'])\n    fr = feature_reduction(**fr_params)\n    cbc = CatBoostClassifier(**cbc_params)\n    pipe = Pipeline([('mi_calc', fr), ('catboost', cbc)])\n    search= GridSearchCV(pipe, param_grid=parameters, cv = cv, \\\n                         scoring= scoring,\\\n                         n_jobs=-1, verbose=1)\n    search.fit(X_train, y_train)\n    searchcv_best(search)\n    y_pred = search.predict(X_test)\n    scores(y_test, y_pred)\n    return search.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-05-17T18:14:38.568251Z","iopub.execute_input":"2022-05-17T18:14:38.568569Z","iopub.status.idle":"2022-05-17T18:14:38.584248Z","shell.execute_reply.started":"2022-05-17T18:14:38.56854Z","shell.execute_reply":"2022-05-17T18:14:38.583295Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The training takes several minutes.","metadata":{}},{"cell_type":"code","source":"%%time\npipe_params = {'mi_calc__max_corr': [0.7, 0.75],\n               'mi_calc__n_neighbors': [5, 19],\n               'catboost__depth': [9, 11]}\n\nfr_params = {'min_mi': 0.0005}\ncbc_params = {'loss_function': 'Logloss', 'logging_level': 'Silent',\\\n              'rsm': 0.25, 'l2_leaf_reg': 4, 'custom_metric': 'AUC'}\n\ncv_best_params = train_pipe_cv(all_percentiles, new_features,\\\n                 parameters=pipe_params, fr_params=fr_params,\\\n                 cbc_params=cbc_params, scoring ='roc_auc')","metadata":{"execution":{"iopub.status.busy":"2022-05-17T18:18:12.824116Z","iopub.execute_input":"2022-05-17T18:18:12.824431Z","iopub.status.idle":"2022-05-17T18:36:42.82845Z","shell.execute_reply.started":"2022-05-17T18:18:12.824398Z","shell.execute_reply":"2022-05-17T18:36:42.826844Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We got a bit of overfitting here, but the test set accuracy is good nevertheless. \n\nI would like to note here that the MI method yields a score for all values of a variable, while a decision tree method splits a variable range and considers its behavior on a part of it. So a feature MI score may be low, but it can be helpful anyway for fine distinctions between our cases.\n\nTo determine what variables were most useful for our training we can check what were feature importances for the model training. Athough the Catboost method is randomized, and for each run we may get a slightly different order.  At first I need to add new learned parameters to the previous ones so I can supply them into my Transformer and CatBoost Estimator.","metadata":{}},{"cell_type":"code","source":"for key in cv_best_params:\n    if key[:3] == 'cat':\n        cbc_params[key.split('__')[1]] = cv_best_params[key]\n    else:\n        fr_params[key.split('__')[1]] = cv_best_params[key]\n        \nfr_cols = feature_reduction(**fr_params)\nfr_cols.fit(all_percentiles[new_features], all_percentiles.troll)\nnew_cols = fr_cols.selected_cols","metadata":{"execution":{"iopub.status.busy":"2022-05-17T18:36:56.847009Z","iopub.execute_input":"2022-05-17T18:36:56.847304Z","iopub.status.idle":"2022-05-17T18:37:16.28478Z","shell.execute_reply.started":"2022-05-17T18:36:56.847272Z","shell.execute_reply":"2022-05-17T18:37:16.283817Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here goes a list of features ordered by computed for this run importance. They permute a little bit with each re-run, although the top ones tend to stay higher in the list. ","metadata":{}},{"cell_type":"code","source":"cbc_best = CatBoostClassifier(**cbc_params)\ncbc_best.fit(all_percentiles[new_cols], all_percentiles.troll.values)\nvar_importance = list(zip(cbc_best.feature_importances_, new_cols))\nvar_importance.sort(key=lambda x:x[0], reverse=True)\nprint(\"A number of variables is \"+str(len(new_cols)))\nvar_importance","metadata":{"execution":{"iopub.status.busy":"2022-05-17T18:37:36.637088Z","iopub.execute_input":"2022-05-17T18:37:36.637372Z","iopub.status.idle":"2022-05-17T18:37:54.607191Z","shell.execute_reply.started":"2022-05-17T18:37:36.637342Z","shell.execute_reply":"2022-05-17T18:37:54.606122Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we see, the most crucial  for troll detection is the way they stick to their instructions. In particular they should include specific words into their texts, so an average word length differs from what normal Americans use. They use a number of ways to increase their audience: links, hashtags, Twitter handles, and it shows. They apparently have guidelines for a post length. I consider it as convenient because we can set up filters for catching the most significant metrics, and then check a whole account activity. \n \nIn addition the suggested features turned out to be not very dependable on languages but mostly on a paid troll account activity. Thus we can apply this work for other languages, and not  limit ourselves to Russian trolls posting English texts.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}